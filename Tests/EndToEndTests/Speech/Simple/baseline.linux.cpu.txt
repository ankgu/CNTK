CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz
    Hardware threads: 24
    Total Memory: 264172964 kB
-------------------------------------------------------------------
=== Running /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple/cntk.cntk currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data RunDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple OutputDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu DeviceId=-1 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: Aug 16 2016 09:41:56
		Last modified date: Fri Aug 12 07:32:43 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 026b1e772b963461e189f8f00aa7ed6951298f84
		Built by philly on f67b30a647de
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
08/16/2016 10:01:21: -------------------------------------------------------------------
08/16/2016 10:01:21: Build info: 

08/16/2016 10:01:21: 		Built time: Aug 16 2016 09:41:56
08/16/2016 10:01:21: 		Last modified date: Fri Aug 12 07:32:43 2016
08/16/2016 10:01:21: 		Build type: release
08/16/2016 10:01:21: 		Build target: GPU
08/16/2016 10:01:21: 		With 1bit-SGD: no
08/16/2016 10:01:21: 		Math lib: mkl
08/16/2016 10:01:21: 		CUDA_PATH: /usr/local/cuda-7.5
08/16/2016 10:01:21: 		CUB_PATH: /usr/local/cub-1.4.1
08/16/2016 10:01:21: 		CUDNN_PATH: /usr/local/cudnn-4.0
08/16/2016 10:01:21: 		Build Branch: HEAD
08/16/2016 10:01:21: 		Build SHA1: 026b1e772b963461e189f8f00aa7ed6951298f84
08/16/2016 10:01:21: 		Built by philly on f67b30a647de
08/16/2016 10:01:21: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
08/16/2016 10:01:21: -------------------------------------------------------------------
08/16/2016 10:01:23: -------------------------------------------------------------------
08/16/2016 10:01:23: GPU info:

08/16/2016 10:01:23: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/16/2016 10:01:23: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/16/2016 10:01:23: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/16/2016 10:01:23: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/16/2016 10:01:23: -------------------------------------------------------------------

08/16/2016 10:01:23: Running on localhost at 2016/08/16 10:01:23
08/16/2016 10:01:23: Command line: 
/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple/cntk.cntk  currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  RunDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu  DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple  OutputDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu  DeviceId=-1  timestamping=true



08/16/2016 10:01:23: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
08/16/2016 10:01:23: command=Simple_Demo:Simple_Demo_Output
DeviceNumber=-1
precision=float
modelPath=$RunDir$/models/simple.dnn
deviceId=$DeviceNumber$
outputNodeNames=ScaledLogLikelihood
traceLevel=1
Simple_Demo=[
    action=train
    SimpleNetworkBuilder=[
        layerSizes=2:50*2:2
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        initValueScale=1.0
        applyMeanVarNorm=true
        uniformInit=true
        needPrior=true
    ]
    SGD=[
        epochSize=0 
        minibatchSize=128
        learningRatesPerSample=0.1
        momentumAsTimeConstant=2500
        dropoutRate=0.0
        maxEpochs=50
        keepCheckPointFiles = true
    ]
    reader=[
        readerType=CNTKTextFormatReader
        file=$DataDir$/SimpleDataTrain_cntk_text.txt
        input = [
            features=[
dim = 2      
                format = "dense"
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
]
Simple_Demo_Output=[
    action=write
    reader=[
        readerType=CNTKTextFormatReader
        file=$DataDir$/SimpleDataTest_cntk_text.txt
        input = [
            features=[
dim = 2 
                format = "dense" 
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
outputPath=$RunDir$/SimpleOutput    
]
currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
RunDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu
DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple
OutputDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu
DeviceId=-1
timestamping=true

08/16/2016 10:01:23: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

08/16/2016 10:01:23: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
08/16/2016 10:01:23: command=Simple_Demo:Simple_Demo_Output
DeviceNumber=-1
precision=float
modelPath=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn
deviceId=-1
outputNodeNames=ScaledLogLikelihood
traceLevel=1
Simple_Demo=[
    action=train
    SimpleNetworkBuilder=[
        layerSizes=2:50*2:2
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        initValueScale=1.0
        applyMeanVarNorm=true
        uniformInit=true
        needPrior=true
    ]
    SGD=[
        epochSize=0 
        minibatchSize=128
        learningRatesPerSample=0.1
        momentumAsTimeConstant=2500
        dropoutRate=0.0
        maxEpochs=50
        keepCheckPointFiles = true
    ]
    reader=[
        readerType=CNTKTextFormatReader
        file=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/SimpleDataTrain_cntk_text.txt
        input = [
            features=[
dim = 2      
                format = "dense"
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
]
Simple_Demo_Output=[
    action=write
    reader=[
        readerType=CNTKTextFormatReader
        file=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/SimpleDataTest_cntk_text.txt
        input = [
            features=[
dim = 2 
                format = "dense" 
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
outputPath=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/SimpleOutput    
]
currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
RunDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu
DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple
OutputDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu
DeviceId=-1
timestamping=true

08/16/2016 10:01:23: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

08/16/2016 10:01:23: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk.cntk:command=Simple_Demo:Simple_Demo_Output
configparameters: cntk.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple
configparameters: cntk.cntk:currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
configparameters: cntk.cntk:DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
configparameters: cntk.cntk:deviceId=-1
configparameters: cntk.cntk:DeviceNumber=-1
configparameters: cntk.cntk:modelPath=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn
configparameters: cntk.cntk:OutputDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu
configparameters: cntk.cntk:outputNodeNames=ScaledLogLikelihood
configparameters: cntk.cntk:precision=float
configparameters: cntk.cntk:RunDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu
configparameters: cntk.cntk:Simple_Demo=[
    action=train
    SimpleNetworkBuilder=[
        layerSizes=2:50*2:2
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        initValueScale=1.0
        applyMeanVarNorm=true
        uniformInit=true
        needPrior=true
    ]
    SGD=[
        epochSize=0 
        minibatchSize=128
        learningRatesPerSample=0.1
        momentumAsTimeConstant=2500
        dropoutRate=0.0
        maxEpochs=50
        keepCheckPointFiles = true
    ]
    reader=[
        readerType=CNTKTextFormatReader
        file=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/SimpleDataTrain_cntk_text.txt
        input = [
            features=[
dim = 2      
                format = "dense"
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
]

configparameters: cntk.cntk:Simple_Demo_Output=[
    action=write
    reader=[
        readerType=CNTKTextFormatReader
        file=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/SimpleDataTest_cntk_text.txt
        input = [
            features=[
dim = 2 
                format = "dense" 
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
outputPath=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/SimpleOutput    
]

configparameters: cntk.cntk:timestamping=true
configparameters: cntk.cntk:traceLevel=1
08/16/2016 10:01:23: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
08/16/2016 10:01:23: Commands: Simple_Demo Simple_Demo_Output
08/16/2016 10:01:23: Precision = "float"
08/16/2016 10:01:23: CNTKModelPath: /tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn
08/16/2016 10:01:23: CNTKCommandTrainInfo: Simple_Demo : 50
08/16/2016 10:01:23: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 50

08/16/2016 10:01:23: ##############################################################################
08/16/2016 10:01:23: #                                                                            #
08/16/2016 10:01:23: # Action "train"                                                             #
08/16/2016 10:01:23: #                                                                            #
08/16/2016 10:01:23: ##############################################################################

08/16/2016 10:01:23: CNTKCommandTrainBegin: Simple_Demo
SimpleNetworkBuilder Using CPU

08/16/2016 10:01:23: Creating virgin network.
Node 'W0' (LearnableParameter operation): Initializing Parameter[50 x 2] <- 0.000000.
Node 'W0' (LearnableParameter operation): Initializing Parameter[50 x 2] <- uniform(seed=1, range=0.050000*1.000000, onCPU=false).
Node 'B0' (LearnableParameter operation): Initializing Parameter[50 x 1] <- 0.000000.
Node 'B0' (LearnableParameter operation): Initializing Parameter[50 x 1] <- 0.000000.
Node 'W1' (LearnableParameter operation): Initializing Parameter[50 x 50] <- 0.000000.
Node 'W1' (LearnableParameter operation): Initializing Parameter[50 x 50] <- uniform(seed=2, range=0.050000*1.000000, onCPU=false).
Node 'B1' (LearnableParameter operation): Initializing Parameter[50 x 1] <- 0.000000.
Node 'B1' (LearnableParameter operation): Initializing Parameter[50 x 1] <- 0.000000.
Node 'W2' (LearnableParameter operation): Initializing Parameter[2 x 50] <- 0.000000.
Node 'W2' (LearnableParameter operation): Initializing Parameter[2 x 50] <- uniform(seed=3, range=0.050000*1.000000, onCPU=false).
Node 'B2' (LearnableParameter operation): Initializing Parameter[2 x 1] <- 0.000000.
Node 'B2' (LearnableParameter operation): Initializing Parameter[2 x 1] <- 0.000000.

Post-processing network...

7 roots:
	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax()
	EvalErrorPrediction = ErrorPrediction()
	InvStdOfFeatures = InvStdDev()
	MeanOfFeatures = Mean()
	PosteriorProb = Softmax()
	Prior = Mean()
	ScaledLogLikelihood = Minus()

Validating network. 25 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [2 x *]
Validating --> W2 = LearnableParameter() :  -> [2 x 50]
Validating --> W1 = LearnableParameter() :  -> [50 x 50]
Validating --> W0 = LearnableParameter() :  -> [50 x 2]
Validating --> features = InputValue() :  -> [2 x *]
Validating --> MeanOfFeatures = Mean (features) : [2 x *] -> [2]
Validating --> InvStdOfFeatures = InvStdDev (features) : [2 x *] -> [2]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization (features, MeanOfFeatures, InvStdOfFeatures) : [2 x *], [2], [2] -> [2 x *]
Validating --> W0*features = Times (W0, MVNormalizedFeatures) : [50 x 2], [2 x *] -> [50 x *]
Validating --> B0 = LearnableParameter() :  -> [50 x 1]
Validating --> W0*features+B0 = Plus (W0*features, B0) : [50 x *], [50 x 1] -> [50 x 1 x *]
Validating --> H1 = Sigmoid (W0*features+B0) : [50 x 1 x *] -> [50 x 1 x *]
Validating --> W1*H1 = Times (W1, H1) : [50 x 50], [50 x 1 x *] -> [50 x 1 x *]
Validating --> B1 = LearnableParameter() :  -> [50 x 1]
Validating --> W1*H1+B1 = Plus (W1*H1, B1) : [50 x 1 x *], [50 x 1] -> [50 x 1 x *]
Validating --> H2 = Sigmoid (W1*H1+B1) : [50 x 1 x *] -> [50 x 1 x *]
Validating --> W2*H1 = Times (W2, H2) : [2 x 50], [50 x 1 x *] -> [2 x 1 x *]
Validating --> B2 = LearnableParameter() :  -> [2 x 1]
Validating --> HLast = Plus (W2*H1, B2) : [2 x 1 x *], [2 x 1] -> [2 x 1 x *]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax (labels, HLast) : [2 x *], [2 x 1 x *] -> [1]
Validating --> EvalErrorPrediction = ErrorPrediction (labels, HLast) : [2 x *], [2 x 1 x *] -> [1]
Validating --> PosteriorProb = Softmax (HLast) : [2 x 1 x *] -> [2 x 1 x *]
Validating --> Prior = Mean (labels) : [2 x *] -> [2]
Validating --> LogOfPrior = Log (Prior) : [2] -> [2]
Validating --> ScaledLogLikelihood = Minus (HLast, LogOfPrior) : [2 x 1 x *], [2] -> [2 x 1 x *]

Validating network. 17 nodes to process in pass 2.


Validating network, final pass.



12 out of 25 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/16/2016 10:01:23: Created model with 25 nodes on CPU.

08/16/2016 10:01:23: Training criterion node(s):
08/16/2016 10:01:23: 	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax

08/16/2016 10:01:23: Evaluation criterion node(s):
08/16/2016 10:01:23: 	EvalErrorPrediction = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 40 matrices, 19 are shared as 8, and 21 are not shared.

	{ W0 : [50 x 2] (gradient)
	  W0*features+B0 : [50 x 1 x *] }
	{ H1 : [50 x 1 x *]
	  W0*features : [50 x *] (gradient) }
	{ W0*features+B0 : [50 x 1 x *] (gradient)
	  W1*H1 : [50 x 1 x *] }
	{ W1 : [50 x 50] (gradient)
	  W1*H1+B1 : [50 x 1 x *] }
	{ H2 : [50 x 1 x *]
	  W1*H1 : [50 x 1 x *] (gradient) }
	{ B0 : [50 x 1] (gradient)
	  H1 : [50 x 1 x *] (gradient)
	  W1*H1+B1 : [50 x 1 x *] (gradient)
	  W2*H1 : [2 x 1 x *] }
	{ HLast : [2 x 1 x *]
	  W2 : [2 x 50] (gradient) }
	{ B1 : [50 x 1] (gradient)
	  H2 : [50 x 1 x *] (gradient)
	  HLast : [2 x 1 x *] (gradient) }


08/16/2016 10:01:23: Training 2802 parameters in 6 out of 6 parameter tensors and 15 nodes with gradient:

08/16/2016 10:01:23: 	Node 'B0' (LearnableParameter operation) : [50 x 1]
08/16/2016 10:01:23: 	Node 'B1' (LearnableParameter operation) : [50 x 1]
08/16/2016 10:01:23: 	Node 'B2' (LearnableParameter operation) : [2 x 1]
08/16/2016 10:01:23: 	Node 'W0' (LearnableParameter operation) : [50 x 2]
08/16/2016 10:01:23: 	Node 'W1' (LearnableParameter operation) : [50 x 50]
08/16/2016 10:01:23: 	Node 'W2' (LearnableParameter operation) : [2 x 50]


08/16/2016 10:01:23: Precomputing --> 3 PreCompute nodes found.

08/16/2016 10:01:23: 	MeanOfFeatures = Mean()
08/16/2016 10:01:23: 	InvStdOfFeatures = InvStdDev()
08/16/2016 10:01:23: 	Prior = Mean()
BlockRandomizer::StartEpoch: epoch 0: frames [0..10000] (first sequence at sample 0), data subset 0 of 1

08/16/2016 10:01:23: Precomputing --> Completed.


08/16/2016 10:01:23: Starting Epoch 1: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 0: frames [0..10000] (first sequence at sample 0), data subset 0 of 1

08/16/2016 10:01:23: Starting minibatch loop.
08/16/2016 10:01:23:  Epoch[ 1 of 50]-Minibatch[   1-  10]: CrossEntropyWithSoftmax = 0.76787934 * 1280; EvalErrorPrediction = 0.50000000 * 1280; time = 0.0559s; samplesPerSecond = 22902.5
08/16/2016 10:01:23:  Epoch[ 1 of 50]-Minibatch[  11-  20]: CrossEntropyWithSoftmax = 0.72901568 * 1280; EvalErrorPrediction = 0.50468750 * 1280; time = 0.0342s; samplesPerSecond = 37411.6
08/16/2016 10:01:23:  Epoch[ 1 of 50]-Minibatch[  21-  30]: CrossEntropyWithSoftmax = 0.69672222 * 1280; EvalErrorPrediction = 0.49765625 * 1280; time = 0.0409s; samplesPerSecond = 31275.2
08/16/2016 10:01:23:  Epoch[ 1 of 50]-Minibatch[  31-  40]: CrossEntropyWithSoftmax = 0.69961834 * 1280; EvalErrorPrediction = 0.46250000 * 1280; time = 0.0394s; samplesPerSecond = 32524.5
08/16/2016 10:01:23:  Epoch[ 1 of 50]-Minibatch[  41-  50]: CrossEntropyWithSoftmax = 0.69465542 * 1280; EvalErrorPrediction = 0.48515625 * 1280; time = 0.0389s; samplesPerSecond = 32930.3
08/16/2016 10:01:23:  Epoch[ 1 of 50]-Minibatch[  51-  60]: CrossEntropyWithSoftmax = 0.69459534 * 1280; EvalErrorPrediction = 0.41171875 * 1280; time = 0.0446s; samplesPerSecond = 28693.8
08/16/2016 10:01:23:  Epoch[ 1 of 50]-Minibatch[  61-  70]: CrossEntropyWithSoftmax = 0.65583725 * 1280; EvalErrorPrediction = 0.41796875 * 1280; time = 0.0491s; samplesPerSecond = 26050.7
08/16/2016 10:01:23: Finished Epoch[ 1 of 50]: [Training] CrossEntropyWithSoftmax = 0.67760273 * 10000; EvalErrorPrediction = 0.43420000 * 10000; totalSamplesSeen = 10000; learningRatePerSample = 0.1; epochTime=0.347591s
08/16/2016 10:01:23: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.1'

08/16/2016 10:01:23: Starting Epoch 2: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 1: frames [10000..20000] (first sequence at sample 10000), data subset 0 of 1

08/16/2016 10:01:23: Starting minibatch loop.
08/16/2016 10:01:23:  Epoch[ 2 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17325155 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0334s; samplesPerSecond = 38316.5
08/16/2016 10:01:23:  Epoch[ 2 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.20231584 * 1280; EvalErrorPrediction = 0.08515625 * 1280; time = 0.0452s; samplesPerSecond = 28316.7
08/16/2016 10:01:23:  Epoch[ 2 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.21851563 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0379s; samplesPerSecond = 33782.0
08/16/2016 10:01:23:  Epoch[ 2 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.19079666 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0378s; samplesPerSecond = 33855.3
08/16/2016 10:01:23:  Epoch[ 2 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.19158516 * 1280; EvalErrorPrediction = 0.08984375 * 1280; time = 0.0392s; samplesPerSecond = 32643.9
08/16/2016 10:01:23:  Epoch[ 2 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.18068714 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0452s; samplesPerSecond = 28341.2
08/16/2016 10:01:24:  Epoch[ 2 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15414639 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0433s; samplesPerSecond = 29534.6
08/16/2016 10:01:24: Finished Epoch[ 2 of 50]: [Training] CrossEntropyWithSoftmax = 0.18580677 * 10000; EvalErrorPrediction = 0.07910000 * 10000; totalSamplesSeen = 20000; learningRatePerSample = 0.1; epochTime=0.322633s
08/16/2016 10:01:24: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.2'

08/16/2016 10:01:24: Starting Epoch 3: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 2: frames [20000..30000] (first sequence at sample 20000), data subset 0 of 1

08/16/2016 10:01:24: Starting minibatch loop.
08/16/2016 10:01:24:  Epoch[ 3 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.20254598 * 1280; EvalErrorPrediction = 0.08593750 * 1280; time = 0.0550s; samplesPerSecond = 23285.0
08/16/2016 10:01:24:  Epoch[ 3 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.18272650 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0760s; samplesPerSecond = 16841.7
08/16/2016 10:01:24:  Epoch[ 3 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16485140 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0526s; samplesPerSecond = 24355.0
08/16/2016 10:01:24:  Epoch[ 3 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.17932067 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0452s; samplesPerSecond = 28296.7
08/16/2016 10:01:24:  Epoch[ 3 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16228499 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0895s; samplesPerSecond = 14296.1
08/16/2016 10:01:24:  Epoch[ 3 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.17208128 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0522s; samplesPerSecond = 24505.1
08/16/2016 10:01:24:  Epoch[ 3 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15790358 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0355s; samplesPerSecond = 36026.9
08/16/2016 10:01:24: Finished Epoch[ 3 of 50]: [Training] CrossEntropyWithSoftmax = 0.17220903 * 10000; EvalErrorPrediction = 0.07540000 * 10000; totalSamplesSeen = 30000; learningRatePerSample = 0.1; epochTime=0.444189s
08/16/2016 10:01:24: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.3'

08/16/2016 10:01:24: Starting Epoch 4: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 3: frames [30000..40000] (first sequence at sample 30000), data subset 0 of 1

08/16/2016 10:01:24: Starting minibatch loop.
08/16/2016 10:01:24:  Epoch[ 4 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15375857 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0397s; samplesPerSecond = 32216.7
08/16/2016 10:01:24:  Epoch[ 4 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17569745 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0443s; samplesPerSecond = 28861.3
08/16/2016 10:01:24:  Epoch[ 4 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16692860 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0892s; samplesPerSecond = 14356.7
08/16/2016 10:01:24:  Epoch[ 4 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.19348469 * 1280; EvalErrorPrediction = 0.08515625 * 1280; time = 0.0360s; samplesPerSecond = 35589.2
08/16/2016 10:01:24:  Epoch[ 4 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16766157 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0384s; samplesPerSecond = 33344.6
08/16/2016 10:01:24:  Epoch[ 4 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15068712 * 1280; EvalErrorPrediction = 0.06718750 * 1280; time = 0.0396s; samplesPerSecond = 32298.8
08/16/2016 10:01:24:  Epoch[ 4 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15202265 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0576s; samplesPerSecond = 22218.0
08/16/2016 10:01:24: Finished Epoch[ 4 of 50]: [Training] CrossEntropyWithSoftmax = 0.16509668 * 10000; EvalErrorPrediction = 0.07510000 * 10000; totalSamplesSeen = 40000; learningRatePerSample = 0.1; epochTime=0.424428s
08/16/2016 10:01:24: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.4'

08/16/2016 10:01:24: Starting Epoch 5: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 4: frames [40000..50000] (first sequence at sample 40000), data subset 0 of 1

08/16/2016 10:01:24: Starting minibatch loop.
08/16/2016 10:01:25:  Epoch[ 5 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15456637 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.2023s; samplesPerSecond = 6326.5
08/16/2016 10:01:25:  Epoch[ 5 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17114824 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.1467s; samplesPerSecond = 8725.2
08/16/2016 10:01:25:  Epoch[ 5 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15907705 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.3598s; samplesPerSecond = 3557.6
08/16/2016 10:01:25:  Epoch[ 5 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15822291 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0427s; samplesPerSecond = 30007.5
08/16/2016 10:01:25:  Epoch[ 5 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.17749352 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0527s; samplesPerSecond = 24295.8
08/16/2016 10:01:25:  Epoch[ 5 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.17140322 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0441s; samplesPerSecond = 28994.7
08/16/2016 10:01:25:  Epoch[ 5 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.17532101 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0855s; samplesPerSecond = 14974.4
08/16/2016 10:01:25: Finished Epoch[ 5 of 50]: [Training] CrossEntropyWithSoftmax = 0.16548099 * 10000; EvalErrorPrediction = 0.07580000 * 10000; totalSamplesSeen = 50000; learningRatePerSample = 0.1; epochTime=0.974623s
08/16/2016 10:01:25: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.5'

08/16/2016 10:01:25: Starting Epoch 6: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 5: frames [50000..60000] (first sequence at sample 50000), data subset 0 of 1

08/16/2016 10:01:25: Starting minibatch loop.
08/16/2016 10:01:25:  Epoch[ 6 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17226899 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0912s; samplesPerSecond = 14039.1
08/16/2016 10:01:26:  Epoch[ 6 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.20439625 * 1280; EvalErrorPrediction = 0.08671875 * 1280; time = 0.0613s; samplesPerSecond = 20885.7
08/16/2016 10:01:26:  Epoch[ 6 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15904801 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0427s; samplesPerSecond = 29982.9
08/16/2016 10:01:26:  Epoch[ 6 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16455998 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0686s; samplesPerSecond = 18653.7
08/16/2016 10:01:26:  Epoch[ 6 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.18057728 * 1280; EvalErrorPrediction = 0.08437500 * 1280; time = 0.0502s; samplesPerSecond = 25488.9
08/16/2016 10:01:26:  Epoch[ 6 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.18234930 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0427s; samplesPerSecond = 29960.4
08/16/2016 10:01:26:  Epoch[ 6 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.18241692 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0397s; samplesPerSecond = 32245.1
08/16/2016 10:01:26: Finished Epoch[ 6 of 50]: [Training] CrossEntropyWithSoftmax = 0.17461414 * 10000; EvalErrorPrediction = 0.07850000 * 10000; totalSamplesSeen = 60000; learningRatePerSample = 0.1; epochTime=0.434805s
08/16/2016 10:01:26: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.6'

08/16/2016 10:01:26: Starting Epoch 7: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 6: frames [60000..70000] (first sequence at sample 60000), data subset 0 of 1

08/16/2016 10:01:26: Starting minibatch loop.
08/16/2016 10:01:26:  Epoch[ 7 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16126106 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0459s; samplesPerSecond = 27865.5
08/16/2016 10:01:26:  Epoch[ 7 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17271674 * 1280; EvalErrorPrediction = 0.08437500 * 1280; time = 0.0398s; samplesPerSecond = 32122.9
08/16/2016 10:01:26:  Epoch[ 7 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15257568 * 1280; EvalErrorPrediction = 0.07890625 * 1280; time = 0.0551s; samplesPerSecond = 23217.0
08/16/2016 10:01:26:  Epoch[ 7 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.13763065 * 1280; EvalErrorPrediction = 0.06093750 * 1280; time = 0.0382s; samplesPerSecond = 33482.4
08/16/2016 10:01:26:  Epoch[ 7 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14629359 * 1280; EvalErrorPrediction = 0.06328125 * 1280; time = 0.0403s; samplesPerSecond = 31795.7
08/16/2016 10:01:26:  Epoch[ 7 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.17639127 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0482s; samplesPerSecond = 26552.7
08/16/2016 10:01:26:  Epoch[ 7 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.17477665 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0654s; samplesPerSecond = 19585.6
08/16/2016 10:01:26: Finished Epoch[ 7 of 50]: [Training] CrossEntropyWithSoftmax = 0.16351710 * 10000; EvalErrorPrediction = 0.07730000 * 10000; totalSamplesSeen = 70000; learningRatePerSample = 0.1; epochTime=0.373545s
08/16/2016 10:01:26: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.7'

08/16/2016 10:01:26: Starting Epoch 8: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 7: frames [70000..80000] (first sequence at sample 70000), data subset 0 of 1

08/16/2016 10:01:26: Starting minibatch loop.
08/16/2016 10:01:26:  Epoch[ 8 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16444900 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0659s; samplesPerSecond = 19420.4
08/16/2016 10:01:26:  Epoch[ 8 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15843136 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0376s; samplesPerSecond = 34057.0
08/16/2016 10:01:26:  Epoch[ 8 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16285515 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0427s; samplesPerSecond = 29967.5
08/16/2016 10:01:26:  Epoch[ 8 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16373024 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0442s; samplesPerSecond = 28945.5
08/16/2016 10:01:26:  Epoch[ 8 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16341076 * 1280; EvalErrorPrediction = 0.08046875 * 1280; time = 0.0399s; samplesPerSecond = 32110.0
08/16/2016 10:01:26:  Epoch[ 8 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.18030205 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0431s; samplesPerSecond = 29691.5
08/16/2016 10:01:27:  Epoch[ 8 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.14125643 * 1280; EvalErrorPrediction = 0.06406250 * 1280; time = 0.0443s; samplesPerSecond = 28916.1
08/16/2016 10:01:27: Finished Epoch[ 8 of 50]: [Training] CrossEntropyWithSoftmax = 0.16304906 * 10000; EvalErrorPrediction = 0.07520000 * 10000; totalSamplesSeen = 80000; learningRatePerSample = 0.1; epochTime=0.359212s
08/16/2016 10:01:27: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.8'

08/16/2016 10:01:27: Starting Epoch 9: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 8: frames [80000..90000] (first sequence at sample 80000), data subset 0 of 1

08/16/2016 10:01:27: Starting minibatch loop.
08/16/2016 10:01:27:  Epoch[ 9 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.18143728 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0381s; samplesPerSecond = 33553.5
08/16/2016 10:01:27:  Epoch[ 9 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.18009384 * 1280; EvalErrorPrediction = 0.07890625 * 1280; time = 0.0443s; samplesPerSecond = 28876.3
08/16/2016 10:01:27:  Epoch[ 9 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.17607446 * 1280; EvalErrorPrediction = 0.08515625 * 1280; time = 0.0501s; samplesPerSecond = 25555.0
08/16/2016 10:01:27:  Epoch[ 9 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.17846565 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0960s; samplesPerSecond = 13333.5
08/16/2016 10:01:27:  Epoch[ 9 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16922598 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0515s; samplesPerSecond = 24838.0
08/16/2016 10:01:27:  Epoch[ 9 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15430174 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0432s; samplesPerSecond = 29623.5
08/16/2016 10:01:27:  Epoch[ 9 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15592928 * 1280; EvalErrorPrediction = 0.06718750 * 1280; time = 0.0485s; samplesPerSecond = 26372.2
08/16/2016 10:01:27: Finished Epoch[ 9 of 50]: [Training] CrossEntropyWithSoftmax = 0.17140103 * 10000; EvalErrorPrediction = 0.07760000 * 10000; totalSamplesSeen = 90000; learningRatePerSample = 0.1; epochTime=0.423098s
08/16/2016 10:01:27: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.9'

08/16/2016 10:01:27: Starting Epoch 10: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 9: frames [90000..100000] (first sequence at sample 90000), data subset 0 of 1

08/16/2016 10:01:27: Starting minibatch loop.
08/16/2016 10:01:27:  Epoch[10 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17334810 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0432s; samplesPerSecond = 29624.8
08/16/2016 10:01:27:  Epoch[10 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15468954 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0426s; samplesPerSecond = 30044.1
08/16/2016 10:01:27:  Epoch[10 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.17982726 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0674s; samplesPerSecond = 18997.6
08/16/2016 10:01:27:  Epoch[10 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16882715 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0707s; samplesPerSecond = 18096.5
08/16/2016 10:01:27:  Epoch[10 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.20348935 * 1280; EvalErrorPrediction = 0.08515625 * 1280; time = 0.0547s; samplesPerSecond = 23411.9
08/16/2016 10:01:27:  Epoch[10 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16315107 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0389s; samplesPerSecond = 32908.3
08/16/2016 10:01:27:  Epoch[10 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15775747 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0370s; samplesPerSecond = 34588.1
08/16/2016 10:01:27: Finished Epoch[10 of 50]: [Training] CrossEntropyWithSoftmax = 0.16880803 * 10000; EvalErrorPrediction = 0.07650000 * 10000; totalSamplesSeen = 100000; learningRatePerSample = 0.1; epochTime=0.396374s
08/16/2016 10:01:27: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.10'

08/16/2016 10:01:27: Starting Epoch 11: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 10: frames [100000..110000] (first sequence at sample 100000), data subset 0 of 1

08/16/2016 10:01:27: Starting minibatch loop.
08/16/2016 10:01:27:  Epoch[11 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17671647 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0436s; samplesPerSecond = 29368.6
08/16/2016 10:01:27:  Epoch[11 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16349745 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0390s; samplesPerSecond = 32813.8
08/16/2016 10:01:28:  Epoch[11 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.17284622 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0443s; samplesPerSecond = 28907.0
08/16/2016 10:01:28:  Epoch[11 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16618757 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0430s; samplesPerSecond = 29774.4
08/16/2016 10:01:28:  Epoch[11 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15637851 * 1280; EvalErrorPrediction = 0.06796875 * 1280; time = 0.0443s; samplesPerSecond = 28906.3
08/16/2016 10:01:28:  Epoch[11 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15405188 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0431s; samplesPerSecond = 29724.6
08/16/2016 10:01:28:  Epoch[11 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16313667 * 1280; EvalErrorPrediction = 0.06953125 * 1280; time = 0.0569s; samplesPerSecond = 22503.1
08/16/2016 10:01:28: Finished Epoch[11 of 50]: [Training] CrossEntropyWithSoftmax = 0.17006791 * 10000; EvalErrorPrediction = 0.07660000 * 10000; totalSamplesSeen = 110000; learningRatePerSample = 0.1; epochTime=0.368894s
08/16/2016 10:01:28: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.11'

08/16/2016 10:01:28: Starting Epoch 12: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 11: frames [110000..120000] (first sequence at sample 110000), data subset 0 of 1

08/16/2016 10:01:28: Starting minibatch loop.
08/16/2016 10:01:28:  Epoch[12 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16747394 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0514s; samplesPerSecond = 24920.7
08/16/2016 10:01:28:  Epoch[12 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16926260 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0729s; samplesPerSecond = 17567.7
08/16/2016 10:01:28:  Epoch[12 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.17724953 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0417s; samplesPerSecond = 30670.4
08/16/2016 10:01:28:  Epoch[12 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16153216 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0368s; samplesPerSecond = 34785.4
08/16/2016 10:01:28:  Epoch[12 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15809488 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0445s; samplesPerSecond = 28744.7
08/16/2016 10:01:28:  Epoch[12 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16424341 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0401s; samplesPerSecond = 31928.2
08/16/2016 10:01:28:  Epoch[12 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.14406061 * 1280; EvalErrorPrediction = 0.06562500 * 1280; time = 0.0444s; samplesPerSecond = 28797.7
08/16/2016 10:01:28: Finished Epoch[12 of 50]: [Training] CrossEntropyWithSoftmax = 0.16305184 * 10000; EvalErrorPrediction = 0.07630000 * 10000; totalSamplesSeen = 120000; learningRatePerSample = 0.1; epochTime=0.371444s
08/16/2016 10:01:28: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.12'

08/16/2016 10:01:28: Starting Epoch 13: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 12: frames [120000..130000] (first sequence at sample 120000), data subset 0 of 1

08/16/2016 10:01:28: Starting minibatch loop.
08/16/2016 10:01:28:  Epoch[13 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15032351 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0434s; samplesPerSecond = 29468.0
08/16/2016 10:01:28:  Epoch[13 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17721360 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0381s; samplesPerSecond = 33570.2
08/16/2016 10:01:28:  Epoch[13 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.17323799 * 1280; EvalErrorPrediction = 0.08906250 * 1280; time = 0.0457s; samplesPerSecond = 28033.3
08/16/2016 10:01:28:  Epoch[13 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15412526 * 1280; EvalErrorPrediction = 0.06796875 * 1280; time = 0.0340s; samplesPerSecond = 37642.6
08/16/2016 10:01:28:  Epoch[13 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14742498 * 1280; EvalErrorPrediction = 0.06484375 * 1280; time = 0.0440s; samplesPerSecond = 29110.8
08/16/2016 10:01:28:  Epoch[13 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.17056408 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0388s; samplesPerSecond = 32971.0
08/16/2016 10:01:28:  Epoch[13 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15463085 * 1280; EvalErrorPrediction = 0.06953125 * 1280; time = 0.0455s; samplesPerSecond = 28141.1
08/16/2016 10:01:28: Finished Epoch[13 of 50]: [Training] CrossEntropyWithSoftmax = 0.16187958 * 10000; EvalErrorPrediction = 0.07470000 * 10000; totalSamplesSeen = 130000; learningRatePerSample = 0.1; epochTime=0.333156s
08/16/2016 10:01:28: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.13'

08/16/2016 10:01:28: Starting Epoch 14: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 13: frames [130000..140000] (first sequence at sample 130000), data subset 0 of 1

08/16/2016 10:01:28: Starting minibatch loop.
08/16/2016 10:01:29:  Epoch[14 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.20428088 * 1280; EvalErrorPrediction = 0.08515625 * 1280; time = 0.0405s; samplesPerSecond = 31579.2
08/16/2016 10:01:29:  Epoch[14 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.20951297 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0399s; samplesPerSecond = 32085.0
08/16/2016 10:01:29:  Epoch[14 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.17470431 * 1280; EvalErrorPrediction = 0.08437500 * 1280; time = 0.0398s; samplesPerSecond = 32198.0
08/16/2016 10:01:29:  Epoch[14 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.20022736 * 1280; EvalErrorPrediction = 0.09140625 * 1280; time = 0.0402s; samplesPerSecond = 31855.1
08/16/2016 10:01:29:  Epoch[14 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.19115577 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0413s; samplesPerSecond = 31009.3
08/16/2016 10:01:29:  Epoch[14 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16565638 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0910s; samplesPerSecond = 14070.0
08/16/2016 10:01:29:  Epoch[14 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15861492 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0490s; samplesPerSecond = 26109.7
08/16/2016 10:01:29: Finished Epoch[14 of 50]: [Training] CrossEntropyWithSoftmax = 0.18525466 * 10000; EvalErrorPrediction = 0.08160000 * 10000; totalSamplesSeen = 140000; learningRatePerSample = 0.1; epochTime=0.39523s
08/16/2016 10:01:29: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.14'

08/16/2016 10:01:29: Starting Epoch 15: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 14: frames [140000..150000] (first sequence at sample 140000), data subset 0 of 1

08/16/2016 10:01:29: Starting minibatch loop.
08/16/2016 10:01:29:  Epoch[15 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17293929 * 1280; EvalErrorPrediction = 0.08828125 * 1280; time = 0.0380s; samplesPerSecond = 33703.7
08/16/2016 10:01:29:  Epoch[15 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.14972337 * 1280; EvalErrorPrediction = 0.06718750 * 1280; time = 0.0447s; samplesPerSecond = 28625.7
08/16/2016 10:01:29:  Epoch[15 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.17388475 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0445s; samplesPerSecond = 28746.0
08/16/2016 10:01:29:  Epoch[15 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16904759 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0432s; samplesPerSecond = 29618.0
08/16/2016 10:01:29:  Epoch[15 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16738753 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0432s; samplesPerSecond = 29653.7
08/16/2016 10:01:29:  Epoch[15 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16737280 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0437s; samplesPerSecond = 29266.5
08/16/2016 10:01:29:  Epoch[15 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15916157 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0431s; samplesPerSecond = 29687.4
08/16/2016 10:01:29: Finished Epoch[15 of 50]: [Training] CrossEntropyWithSoftmax = 0.16554249 * 10000; EvalErrorPrediction = 0.07680000 * 10000; totalSamplesSeen = 150000; learningRatePerSample = 0.1; epochTime=0.348812s
08/16/2016 10:01:29: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.15'

08/16/2016 10:01:29: Starting Epoch 16: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 15: frames [150000..160000] (first sequence at sample 150000), data subset 0 of 1

08/16/2016 10:01:29: Starting minibatch loop.
08/16/2016 10:01:29:  Epoch[16 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17063624 * 1280; EvalErrorPrediction = 0.06796875 * 1280; time = 0.0392s; samplesPerSecond = 32633.1
08/16/2016 10:01:29:  Epoch[16 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16444885 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0557s; samplesPerSecond = 22991.8
08/16/2016 10:01:29:  Epoch[16 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.17523324 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0819s; samplesPerSecond = 15634.2
08/16/2016 10:01:29:  Epoch[16 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.17308168 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0824s; samplesPerSecond = 15535.1
08/16/2016 10:01:30:  Epoch[16 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.18644814 * 1280; EvalErrorPrediction = 0.08671875 * 1280; time = 0.0654s; samplesPerSecond = 19557.5
08/16/2016 10:01:30:  Epoch[16 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.17276363 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0633s; samplesPerSecond = 20236.5
08/16/2016 10:01:30:  Epoch[16 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.17704020 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0376s; samplesPerSecond = 34063.4
08/16/2016 10:01:30: Finished Epoch[16 of 50]: [Training] CrossEntropyWithSoftmax = 0.17266423 * 10000; EvalErrorPrediction = 0.07770000 * 10000; totalSamplesSeen = 160000; learningRatePerSample = 0.1; epochTime=0.478754s
08/16/2016 10:01:30: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.16'

08/16/2016 10:01:30: Starting Epoch 17: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 16: frames [160000..170000] (first sequence at sample 160000), data subset 0 of 1

08/16/2016 10:01:30: Starting minibatch loop.
08/16/2016 10:01:30:  Epoch[17 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15754168 * 1280; EvalErrorPrediction = 0.07890625 * 1280; time = 0.1793s; samplesPerSecond = 7138.0
08/16/2016 10:01:30:  Epoch[17 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16700337 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.1077s; samplesPerSecond = 11879.5
08/16/2016 10:01:30:  Epoch[17 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.18876448 * 1280; EvalErrorPrediction = 0.09453125 * 1280; time = 0.0768s; samplesPerSecond = 16674.3
08/16/2016 10:01:30:  Epoch[17 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16264739 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0385s; samplesPerSecond = 33215.7
08/16/2016 10:01:30:  Epoch[17 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15761566 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0447s; samplesPerSecond = 28630.2
08/16/2016 10:01:30:  Epoch[17 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15861073 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0398s; samplesPerSecond = 32124.5
08/16/2016 10:01:30:  Epoch[17 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15868063 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0485s; samplesPerSecond = 26379.2
08/16/2016 10:01:30: Finished Epoch[17 of 50]: [Training] CrossEntropyWithSoftmax = 0.16474818 * 10000; EvalErrorPrediction = 0.07880000 * 10000; totalSamplesSeen = 170000; learningRatePerSample = 0.1; epochTime=0.576909s
08/16/2016 10:01:30: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.17'

08/16/2016 10:01:30: Starting Epoch 18: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 17: frames [170000..180000] (first sequence at sample 170000), data subset 0 of 1

08/16/2016 10:01:30: Starting minibatch loop.
08/16/2016 10:01:30:  Epoch[18 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15372933 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0437s; samplesPerSecond = 29299.3
08/16/2016 10:01:30:  Epoch[18 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16174213 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0446s; samplesPerSecond = 28707.9
08/16/2016 10:01:30:  Epoch[18 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16805205 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0470s; samplesPerSecond = 27244.5
08/16/2016 10:01:30:  Epoch[18 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15750542 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0433s; samplesPerSecond = 29529.1
08/16/2016 10:01:30:  Epoch[18 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14848323 * 1280; EvalErrorPrediction = 0.06796875 * 1280; time = 0.0404s; samplesPerSecond = 31679.2
08/16/2016 10:01:31:  Epoch[18 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15929031 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0445s; samplesPerSecond = 28777.0
08/16/2016 10:01:31:  Epoch[18 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15710373 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0446s; samplesPerSecond = 28711.1
08/16/2016 10:01:31: Finished Epoch[18 of 50]: [Training] CrossEntropyWithSoftmax = 0.15840823 * 10000; EvalErrorPrediction = 0.07410000 * 10000; totalSamplesSeen = 180000; learningRatePerSample = 0.1; epochTime=0.346991s
08/16/2016 10:01:31: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.18'

08/16/2016 10:01:31: Starting Epoch 19: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 18: frames [180000..190000] (first sequence at sample 180000), data subset 0 of 1

08/16/2016 10:01:31: Starting minibatch loop.
08/16/2016 10:01:31:  Epoch[19 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15729398 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0430s; samplesPerSecond = 29783.4
08/16/2016 10:01:31:  Epoch[19 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15066613 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0593s; samplesPerSecond = 21576.1
08/16/2016 10:01:31:  Epoch[19 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16931269 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0836s; samplesPerSecond = 15312.7
08/16/2016 10:01:31:  Epoch[19 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15056415 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0434s; samplesPerSecond = 29503.3
08/16/2016 10:01:31:  Epoch[19 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.18283534 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0603s; samplesPerSecond = 21211.4
08/16/2016 10:01:31:  Epoch[19 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16821423 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0438s; samplesPerSecond = 29250.5
08/16/2016 10:01:31:  Epoch[19 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.17998962 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0498s; samplesPerSecond = 25686.3
08/16/2016 10:01:31: Finished Epoch[19 of 50]: [Training] CrossEntropyWithSoftmax = 0.16611069 * 10000; EvalErrorPrediction = 0.07720000 * 10000; totalSamplesSeen = 190000; learningRatePerSample = 0.1; epochTime=0.423192s
08/16/2016 10:01:31: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.19'

08/16/2016 10:01:31: Starting Epoch 20: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 19: frames [190000..200000] (first sequence at sample 190000), data subset 0 of 1

08/16/2016 10:01:31: Starting minibatch loop.
08/16/2016 10:01:31:  Epoch[20 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16058820 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0432s; samplesPerSecond = 29600.9
08/16/2016 10:01:31:  Epoch[20 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15364984 * 1280; EvalErrorPrediction = 0.06640625 * 1280; time = 0.0376s; samplesPerSecond = 34047.1
08/16/2016 10:01:31:  Epoch[20 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15540874 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0492s; samplesPerSecond = 26030.5
08/16/2016 10:01:31:  Epoch[20 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16862626 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0389s; samplesPerSecond = 32911.7
08/16/2016 10:01:31:  Epoch[20 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15999022 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0447s; samplesPerSecond = 28637.9
08/16/2016 10:01:31:  Epoch[20 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15753345 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0473s; samplesPerSecond = 27074.5
08/16/2016 10:01:31:  Epoch[20 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16873503 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0697s; samplesPerSecond = 18372.6
08/16/2016 10:01:31: Finished Epoch[20 of 50]: [Training] CrossEntropyWithSoftmax = 0.15916772 * 10000; EvalErrorPrediction = 0.07450000 * 10000; totalSamplesSeen = 200000; learningRatePerSample = 0.1; epochTime=0.400141s
08/16/2016 10:01:31: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.20'

08/16/2016 10:01:31: Starting Epoch 21: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 20: frames [200000..210000] (first sequence at sample 200000), data subset 0 of 1

08/16/2016 10:01:31: Starting minibatch loop.
08/16/2016 10:01:32:  Epoch[21 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16457530 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0664s; samplesPerSecond = 19284.7
08/16/2016 10:01:32:  Epoch[21 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.14965638 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0548s; samplesPerSecond = 23348.7
08/16/2016 10:01:32:  Epoch[21 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15823965 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0479s; samplesPerSecond = 26744.1
08/16/2016 10:01:32:  Epoch[21 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.14598193 * 1280; EvalErrorPrediction = 0.06484375 * 1280; time = 0.0719s; samplesPerSecond = 17803.2
08/16/2016 10:01:32:  Epoch[21 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16491952 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0525s; samplesPerSecond = 24368.0
08/16/2016 10:01:32:  Epoch[21 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.17231426 * 1280; EvalErrorPrediction = 0.08671875 * 1280; time = 0.0615s; samplesPerSecond = 20813.0
08/16/2016 10:01:32:  Epoch[21 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16371145 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0525s; samplesPerSecond = 24359.1
08/16/2016 10:01:32: Finished Epoch[21 of 50]: [Training] CrossEntropyWithSoftmax = 0.16036295 * 10000; EvalErrorPrediction = 0.07640000 * 10000; totalSamplesSeen = 210000; learningRatePerSample = 0.1; epochTime=0.44493s
08/16/2016 10:01:32: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.21'

08/16/2016 10:01:32: Starting Epoch 22: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 21: frames [210000..220000] (first sequence at sample 210000), data subset 0 of 1

08/16/2016 10:01:32: Starting minibatch loop.
08/16/2016 10:01:32:  Epoch[22 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15611972 * 1280; EvalErrorPrediction = 0.06796875 * 1280; time = 0.0489s; samplesPerSecond = 26183.9
08/16/2016 10:01:32:  Epoch[22 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.13785256 * 1280; EvalErrorPrediction = 0.06015625 * 1280; time = 0.0385s; samplesPerSecond = 33234.7
08/16/2016 10:01:32:  Epoch[22 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16535847 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0396s; samplesPerSecond = 32353.5
08/16/2016 10:01:32:  Epoch[22 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16503291 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0447s; samplesPerSecond = 28652.0
08/16/2016 10:01:32:  Epoch[22 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15546598 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0395s; samplesPerSecond = 32438.7
08/16/2016 10:01:32:  Epoch[22 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15233245 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0384s; samplesPerSecond = 33342.0
08/16/2016 10:01:32:  Epoch[22 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.19573660 * 1280; EvalErrorPrediction = 0.09062500 * 1280; time = 0.0446s; samplesPerSecond = 28704.1
08/16/2016 10:01:32: Finished Epoch[22 of 50]: [Training] CrossEntropyWithSoftmax = 0.16261056 * 10000; EvalErrorPrediction = 0.07580000 * 10000; totalSamplesSeen = 220000; learningRatePerSample = 0.1; epochTime=0.373912s
08/16/2016 10:01:32: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.22'

08/16/2016 10:01:32: Starting Epoch 23: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 22: frames [220000..230000] (first sequence at sample 220000), data subset 0 of 1

08/16/2016 10:01:32: Starting minibatch loop.
08/16/2016 10:01:32:  Epoch[23 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16898190 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0999s; samplesPerSecond = 12818.3
08/16/2016 10:01:32:  Epoch[23 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17614683 * 1280; EvalErrorPrediction = 0.08046875 * 1280; time = 0.0663s; samplesPerSecond = 19309.7
08/16/2016 10:01:32:  Epoch[23 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16341057 * 1280; EvalErrorPrediction = 0.07890625 * 1280; time = 0.0440s; samplesPerSecond = 29059.9
08/16/2016 10:01:33:  Epoch[23 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16389289 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0441s; samplesPerSecond = 29057.2
08/16/2016 10:01:33:  Epoch[23 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15507503 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0453s; samplesPerSecond = 28228.7
08/16/2016 10:01:33:  Epoch[23 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16150293 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0417s; samplesPerSecond = 30677.1
08/16/2016 10:01:33:  Epoch[23 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15201712 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0442s; samplesPerSecond = 28980.3
08/16/2016 10:01:33: Finished Epoch[23 of 50]: [Training] CrossEntropyWithSoftmax = 0.16328606 * 10000; EvalErrorPrediction = 0.07740000 * 10000; totalSamplesSeen = 230000; learningRatePerSample = 0.1; epochTime=0.466598s
08/16/2016 10:01:33: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.23'

08/16/2016 10:01:33: Starting Epoch 24: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 23: frames [230000..240000] (first sequence at sample 230000), data subset 0 of 1

08/16/2016 10:01:33: Starting minibatch loop.
08/16/2016 10:01:33:  Epoch[24 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.18361137 * 1280; EvalErrorPrediction = 0.08750000 * 1280; time = 0.0788s; samplesPerSecond = 16244.3
08/16/2016 10:01:33:  Epoch[24 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.18937519 * 1280; EvalErrorPrediction = 0.08984375 * 1280; time = 0.0446s; samplesPerSecond = 28729.8
08/16/2016 10:01:33:  Epoch[24 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15248947 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.1197s; samplesPerSecond = 10697.8
08/16/2016 10:01:33:  Epoch[24 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.13488998 * 1280; EvalErrorPrediction = 0.05703125 * 1280; time = 0.0541s; samplesPerSecond = 23662.1
08/16/2016 10:01:33:  Epoch[24 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.18253484 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0394s; samplesPerSecond = 32465.1
08/16/2016 10:01:33:  Epoch[24 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.13559303 * 1280; EvalErrorPrediction = 0.05859375 * 1280; time = 0.0459s; samplesPerSecond = 27891.6
08/16/2016 10:01:33:  Epoch[24 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16215487 * 1280; EvalErrorPrediction = 0.07890625 * 1280; time = 0.0615s; samplesPerSecond = 20812.3
08/16/2016 10:01:33: Finished Epoch[24 of 50]: [Training] CrossEntropyWithSoftmax = 0.16349951 * 10000; EvalErrorPrediction = 0.07620000 * 10000; totalSamplesSeen = 240000; learningRatePerSample = 0.1; epochTime=0.488078s
08/16/2016 10:01:33: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.24'

08/16/2016 10:01:33: Starting Epoch 25: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 24: frames [240000..250000] (first sequence at sample 240000), data subset 0 of 1

08/16/2016 10:01:33: Starting minibatch loop.
08/16/2016 10:01:33:  Epoch[25 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.14410825 * 1280; EvalErrorPrediction = 0.06015625 * 1280; time = 0.0421s; samplesPerSecond = 30424.0
08/16/2016 10:01:33:  Epoch[25 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17700341 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0454s; samplesPerSecond = 28213.7
08/16/2016 10:01:33:  Epoch[25 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.14907324 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0448s; samplesPerSecond = 28597.0
08/16/2016 10:01:33:  Epoch[25 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16086655 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0462s; samplesPerSecond = 27732.6
08/16/2016 10:01:33:  Epoch[25 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.17219219 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0449s; samplesPerSecond = 28519.2
08/16/2016 10:01:33:  Epoch[25 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.17564411 * 1280; EvalErrorPrediction = 0.08437500 * 1280; time = 0.0464s; samplesPerSecond = 27578.5
08/16/2016 10:01:34:  Epoch[25 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.17609806 * 1280; EvalErrorPrediction = 0.08593750 * 1280; time = 0.0444s; samplesPerSecond = 28831.4
08/16/2016 10:01:34: Finished Epoch[25 of 50]: [Training] CrossEntropyWithSoftmax = 0.16392158 * 10000; EvalErrorPrediction = 0.07540000 * 10000; totalSamplesSeen = 250000; learningRatePerSample = 0.1; epochTime=0.358302s
08/16/2016 10:01:34: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.25'

08/16/2016 10:01:34: Starting Epoch 26: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 25: frames [250000..260000] (first sequence at sample 250000), data subset 0 of 1

08/16/2016 10:01:34: Starting minibatch loop.
08/16/2016 10:01:34:  Epoch[26 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.14948860 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0446s; samplesPerSecond = 28684.1
08/16/2016 10:01:34:  Epoch[26 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.18475274 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0605s; samplesPerSecond = 21142.3
08/16/2016 10:01:34:  Epoch[26 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16298699 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0748s; samplesPerSecond = 17104.1
08/16/2016 10:01:34:  Epoch[26 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.14671760 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0511s; samplesPerSecond = 25062.2
08/16/2016 10:01:34:  Epoch[26 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16988988 * 1280; EvalErrorPrediction = 0.08593750 * 1280; time = 0.0839s; samplesPerSecond = 15262.4
08/16/2016 10:01:34:  Epoch[26 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.12634602 * 1280; EvalErrorPrediction = 0.05156250 * 1280; time = 0.0670s; samplesPerSecond = 19114.2
08/16/2016 10:01:34:  Epoch[26 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16665192 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0363s; samplesPerSecond = 35271.4
08/16/2016 10:01:34: Finished Epoch[26 of 50]: [Training] CrossEntropyWithSoftmax = 0.15836489 * 10000; EvalErrorPrediction = 0.07440000 * 10000; totalSamplesSeen = 260000; learningRatePerSample = 0.1; epochTime=0.46373s
08/16/2016 10:01:34: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.26'

08/16/2016 10:01:34: Starting Epoch 27: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 26: frames [260000..270000] (first sequence at sample 260000), data subset 0 of 1

08/16/2016 10:01:34: Starting minibatch loop.
08/16/2016 10:01:34:  Epoch[27 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17073995 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0466s; samplesPerSecond = 27483.7
08/16/2016 10:01:34:  Epoch[27 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17057620 * 1280; EvalErrorPrediction = 0.08515625 * 1280; time = 0.0473s; samplesPerSecond = 27081.3
08/16/2016 10:01:34:  Epoch[27 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.14733498 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0450s; samplesPerSecond = 28415.4
08/16/2016 10:01:34:  Epoch[27 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15325294 * 1280; EvalErrorPrediction = 0.06328125 * 1280; time = 0.0418s; samplesPerSecond = 30633.0
08/16/2016 10:01:34:  Epoch[27 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15593843 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0410s; samplesPerSecond = 31228.7
08/16/2016 10:01:34:  Epoch[27 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15706940 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0426s; samplesPerSecond = 30066.0
08/16/2016 10:01:34:  Epoch[27 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.17286139 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0464s; samplesPerSecond = 27586.8
08/16/2016 10:01:34: Finished Epoch[27 of 50]: [Training] CrossEntropyWithSoftmax = 0.16107898 * 10000; EvalErrorPrediction = 0.07440000 * 10000; totalSamplesSeen = 270000; learningRatePerSample = 0.1; epochTime=0.356518s
08/16/2016 10:01:34: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.27'

08/16/2016 10:01:34: Starting Epoch 28: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 27: frames [270000..280000] (first sequence at sample 270000), data subset 0 of 1

08/16/2016 10:01:34: Starting minibatch loop.
08/16/2016 10:01:34:  Epoch[28 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16945839 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0425s; samplesPerSecond = 30107.0
08/16/2016 10:01:34:  Epoch[28 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.18443935 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0503s; samplesPerSecond = 25456.9
08/16/2016 10:01:35:  Epoch[28 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16735666 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0531s; samplesPerSecond = 24127.3
08/16/2016 10:01:35:  Epoch[28 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15123658 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0604s; samplesPerSecond = 21206.8
08/16/2016 10:01:35:  Epoch[28 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14052629 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.2817s; samplesPerSecond = 4544.4
08/16/2016 10:01:35:  Epoch[28 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15429277 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.1074s; samplesPerSecond = 11915.3
08/16/2016 10:01:35:  Epoch[28 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16686211 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0413s; samplesPerSecond = 30981.5
08/16/2016 10:01:35: Finished Epoch[28 of 50]: [Training] CrossEntropyWithSoftmax = 0.16312491 * 10000; EvalErrorPrediction = 0.07620000 * 10000; totalSamplesSeen = 280000; learningRatePerSample = 0.1; epochTime=0.694364s
08/16/2016 10:01:35: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.28'

08/16/2016 10:01:35: Starting Epoch 29: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 28: frames [280000..290000] (first sequence at sample 280000), data subset 0 of 1

08/16/2016 10:01:35: Starting minibatch loop.
08/16/2016 10:01:35:  Epoch[29 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16114533 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0388s; samplesPerSecond = 33014.4
08/16/2016 10:01:35:  Epoch[29 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.13027835 * 1280; EvalErrorPrediction = 0.05703125 * 1280; time = 0.0399s; samplesPerSecond = 32068.1
08/16/2016 10:01:35:  Epoch[29 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16984265 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0440s; samplesPerSecond = 29112.7
08/16/2016 10:01:35:  Epoch[29 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15046000 * 1280; EvalErrorPrediction = 0.06640625 * 1280; time = 0.0503s; samplesPerSecond = 25462.5
08/16/2016 10:01:35:  Epoch[29 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16504669 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0997s; samplesPerSecond = 12842.3
08/16/2016 10:01:35:  Epoch[29 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.14679303 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0419s; samplesPerSecond = 30554.0
08/16/2016 10:01:35:  Epoch[29 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.17150068 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0443s; samplesPerSecond = 28924.6
08/16/2016 10:01:36: Finished Epoch[29 of 50]: [Training] CrossEntropyWithSoftmax = 0.16135596 * 10000; EvalErrorPrediction = 0.07450000 * 10000; totalSamplesSeen = 290000; learningRatePerSample = 0.1; epochTime=0.402828s
08/16/2016 10:01:36: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.29'

08/16/2016 10:01:36: Starting Epoch 30: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 29: frames [290000..300000] (first sequence at sample 290000), data subset 0 of 1

08/16/2016 10:01:36: Starting minibatch loop.
08/16/2016 10:01:36:  Epoch[30 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16378827 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0661s; samplesPerSecond = 19366.7
08/16/2016 10:01:36:  Epoch[30 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15350196 * 1280; EvalErrorPrediction = 0.06562500 * 1280; time = 0.0480s; samplesPerSecond = 26691.1
08/16/2016 10:01:36:  Epoch[30 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16200140 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0450s; samplesPerSecond = 28459.0
08/16/2016 10:01:36:  Epoch[30 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.17997212 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.1372s; samplesPerSecond = 9332.8
08/16/2016 10:01:36:  Epoch[30 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.17179399 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0866s; samplesPerSecond = 14778.7
08/16/2016 10:01:36:  Epoch[30 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16910982 * 1280; EvalErrorPrediction = 0.06953125 * 1280; time = 0.0580s; samplesPerSecond = 22077.0
08/16/2016 10:01:36:  Epoch[30 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.17447052 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0579s; samplesPerSecond = 22108.2
08/16/2016 10:01:36: Finished Epoch[30 of 50]: [Training] CrossEntropyWithSoftmax = 0.16685378 * 10000; EvalErrorPrediction = 0.07510000 * 10000; totalSamplesSeen = 300000; learningRatePerSample = 0.1; epochTime=0.557698s
08/16/2016 10:01:36: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.30'

08/16/2016 10:01:36: Starting Epoch 31: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 30: frames [300000..310000] (first sequence at sample 300000), data subset 0 of 1

08/16/2016 10:01:36: Starting minibatch loop.
08/16/2016 10:01:36:  Epoch[31 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17310435 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0449s; samplesPerSecond = 28483.1
08/16/2016 10:01:36:  Epoch[31 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17225746 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0404s; samplesPerSecond = 31655.0
08/16/2016 10:01:36:  Epoch[31 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.19278526 * 1280; EvalErrorPrediction = 0.08593750 * 1280; time = 0.0452s; samplesPerSecond = 28319.8
08/16/2016 10:01:36:  Epoch[31 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.14848056 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0415s; samplesPerSecond = 30870.2
08/16/2016 10:01:36:  Epoch[31 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.17797055 * 1280; EvalErrorPrediction = 0.08515625 * 1280; time = 0.0454s; samplesPerSecond = 28196.3
08/16/2016 10:01:36:  Epoch[31 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.14993353 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0454s; samplesPerSecond = 28195.7
08/16/2016 10:01:36:  Epoch[31 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15609417 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0538s; samplesPerSecond = 23798.5
08/16/2016 10:01:36: Finished Epoch[31 of 50]: [Training] CrossEntropyWithSoftmax = 0.16720474 * 10000; EvalErrorPrediction = 0.08070000 * 10000; totalSamplesSeen = 310000; learningRatePerSample = 0.1; epochTime=0.355191s
08/16/2016 10:01:36: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.31'

08/16/2016 10:01:36: Starting Epoch 32: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 31: frames [310000..320000] (first sequence at sample 310000), data subset 0 of 1

08/16/2016 10:01:36: Starting minibatch loop.
08/16/2016 10:01:36:  Epoch[32 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16825123 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0530s; samplesPerSecond = 24132.3
08/16/2016 10:01:37:  Epoch[32 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15172620 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0369s; samplesPerSecond = 34690.2
08/16/2016 10:01:37:  Epoch[32 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.14193788 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0475s; samplesPerSecond = 26925.3
08/16/2016 10:01:37:  Epoch[32 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16006799 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0474s; samplesPerSecond = 26984.3
08/16/2016 10:01:37:  Epoch[32 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15443039 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0455s; samplesPerSecond = 28146.7
08/16/2016 10:01:37:  Epoch[32 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.17014613 * 1280; EvalErrorPrediction = 0.07890625 * 1280; time = 0.1717s; samplesPerSecond = 7453.8
08/16/2016 10:01:37:  Epoch[32 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15410852 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.1041s; samplesPerSecond = 12293.2
08/16/2016 10:01:37: Finished Epoch[32 of 50]: [Training] CrossEntropyWithSoftmax = 0.15899670 * 10000; EvalErrorPrediction = 0.07470000 * 10000; totalSamplesSeen = 320000; learningRatePerSample = 0.1; epochTime=0.546557s
08/16/2016 10:01:37: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.32'

08/16/2016 10:01:37: Starting Epoch 33: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 32: frames [320000..330000] (first sequence at sample 320000), data subset 0 of 1

08/16/2016 10:01:37: Starting minibatch loop.
08/16/2016 10:01:37:  Epoch[33 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16432738 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0429s; samplesPerSecond = 29822.9
08/16/2016 10:01:37:  Epoch[33 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17564251 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0444s; samplesPerSecond = 28815.2
08/16/2016 10:01:37:  Epoch[33 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15467417 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0438s; samplesPerSecond = 29229.8
08/16/2016 10:01:37:  Epoch[33 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15989141 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0672s; samplesPerSecond = 19041.4
08/16/2016 10:01:37:  Epoch[33 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14214611 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0540s; samplesPerSecond = 23689.2
08/16/2016 10:01:37:  Epoch[33 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15155411 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0600s; samplesPerSecond = 21328.7
08/16/2016 10:01:37:  Epoch[33 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.17212152 * 1280; EvalErrorPrediction = 0.08906250 * 1280; time = 0.0434s; samplesPerSecond = 29484.9
08/16/2016 10:01:37: Finished Epoch[33 of 50]: [Training] CrossEntropyWithSoftmax = 0.15995256 * 10000; EvalErrorPrediction = 0.07600000 * 10000; totalSamplesSeen = 330000; learningRatePerSample = 0.1; epochTime=0.394294s
08/16/2016 10:01:37: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.33'

08/16/2016 10:01:37: Starting Epoch 34: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 33: frames [330000..340000] (first sequence at sample 330000), data subset 0 of 1

08/16/2016 10:01:37: Starting minibatch loop.
08/16/2016 10:01:37:  Epoch[34 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.13921938 * 1280; EvalErrorPrediction = 0.06640625 * 1280; time = 0.0397s; samplesPerSecond = 32243.4
08/16/2016 10:01:37:  Epoch[34 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16276331 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0429s; samplesPerSecond = 29820.8
08/16/2016 10:01:37:  Epoch[34 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.17090068 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0390s; samplesPerSecond = 32819.7
08/16/2016 10:01:38:  Epoch[34 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.14848137 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0384s; samplesPerSecond = 33343.8
08/16/2016 10:01:38:  Epoch[34 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16527195 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0429s; samplesPerSecond = 29836.1
08/16/2016 10:01:38:  Epoch[34 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.17159867 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0438s; samplesPerSecond = 29234.4
08/16/2016 10:01:38:  Epoch[34 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16636944 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0412s; samplesPerSecond = 31051.4
08/16/2016 10:01:38: Finished Epoch[34 of 50]: [Training] CrossEntropyWithSoftmax = 0.15936174 * 10000; EvalErrorPrediction = 0.07480000 * 10000; totalSamplesSeen = 340000; learningRatePerSample = 0.1; epochTime=0.340246s
08/16/2016 10:01:38: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.34'

08/16/2016 10:01:38: Starting Epoch 35: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 34: frames [340000..350000] (first sequence at sample 340000), data subset 0 of 1

08/16/2016 10:01:38: Starting minibatch loop.
08/16/2016 10:01:38:  Epoch[35 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16906821 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0434s; samplesPerSecond = 29472.7
08/16/2016 10:01:38:  Epoch[35 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.14418758 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0956s; samplesPerSecond = 13386.5
08/16/2016 10:01:38:  Epoch[35 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15848689 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0435s; samplesPerSecond = 29454.4
08/16/2016 10:01:38:  Epoch[35 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16318011 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0669s; samplesPerSecond = 19145.1
08/16/2016 10:01:38:  Epoch[35 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15352063 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0565s; samplesPerSecond = 22646.4
08/16/2016 10:01:38:  Epoch[35 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.14742370 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0477s; samplesPerSecond = 26823.7
08/16/2016 10:01:38:  Epoch[35 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16554413 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0648s; samplesPerSecond = 19741.5
08/16/2016 10:01:38: Finished Epoch[35 of 50]: [Training] CrossEntropyWithSoftmax = 0.15775406 * 10000; EvalErrorPrediction = 0.07640000 * 10000; totalSamplesSeen = 350000; learningRatePerSample = 0.1; epochTime=0.519044s
08/16/2016 10:01:38: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.35'

08/16/2016 10:01:38: Starting Epoch 36: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 35: frames [350000..360000] (first sequence at sample 350000), data subset 0 of 1

08/16/2016 10:01:38: Starting minibatch loop.
08/16/2016 10:01:38:  Epoch[36 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15373969 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0543s; samplesPerSecond = 23581.9
08/16/2016 10:01:38:  Epoch[36 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.19182336 * 1280; EvalErrorPrediction = 0.08671875 * 1280; time = 0.0443s; samplesPerSecond = 28865.2
08/16/2016 10:01:38:  Epoch[36 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15158508 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0582s; samplesPerSecond = 21987.5
08/16/2016 10:01:38:  Epoch[36 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15708618 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0432s; samplesPerSecond = 29614.5
08/16/2016 10:01:38:  Epoch[36 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14619446 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0508s; samplesPerSecond = 25195.9
08/16/2016 10:01:39:  Epoch[36 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.13910179 * 1280; EvalErrorPrediction = 0.06718750 * 1280; time = 0.0409s; samplesPerSecond = 31288.2
08/16/2016 10:01:39:  Epoch[36 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15226822 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0412s; samplesPerSecond = 31101.2
08/16/2016 10:01:39: Finished Epoch[36 of 50]: [Training] CrossEntropyWithSoftmax = 0.15698855 * 10000; EvalErrorPrediction = 0.07530000 * 10000; totalSamplesSeen = 360000; learningRatePerSample = 0.1; epochTime=0.373523s
08/16/2016 10:01:39: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.36'

08/16/2016 10:01:39: Starting Epoch 37: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 36: frames [360000..370000] (first sequence at sample 360000), data subset 0 of 1

08/16/2016 10:01:39: Starting minibatch loop.
08/16/2016 10:01:39:  Epoch[37 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17271128 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0444s; samplesPerSecond = 28812.0
08/16/2016 10:01:39:  Epoch[37 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15075910 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0612s; samplesPerSecond = 20908.2
08/16/2016 10:01:39:  Epoch[37 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15440271 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.1014s; samplesPerSecond = 12622.7
08/16/2016 10:01:39:  Epoch[37 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.12932649 * 1280; EvalErrorPrediction = 0.06015625 * 1280; time = 0.0436s; samplesPerSecond = 29370.6
08/16/2016 10:01:39:  Epoch[37 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16559262 * 1280; EvalErrorPrediction = 0.08750000 * 1280; time = 0.0386s; samplesPerSecond = 33137.4
08/16/2016 10:01:39:  Epoch[37 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.14702020 * 1280; EvalErrorPrediction = 0.06406250 * 1280; time = 0.0496s; samplesPerSecond = 25815.3
08/16/2016 10:01:39:  Epoch[37 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16273041 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0478s; samplesPerSecond = 26772.1
08/16/2016 10:01:39: Finished Epoch[37 of 50]: [Training] CrossEntropyWithSoftmax = 0.15447834 * 10000; EvalErrorPrediction = 0.07370000 * 10000; totalSamplesSeen = 370000; learningRatePerSample = 0.1; epochTime=0.445647s
08/16/2016 10:01:39: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.37'

08/16/2016 10:01:39: Starting Epoch 38: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 37: frames [370000..380000] (first sequence at sample 370000), data subset 0 of 1

08/16/2016 10:01:39: Starting minibatch loop.
08/16/2016 10:01:39:  Epoch[38 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17086862 * 1280; EvalErrorPrediction = 0.08593750 * 1280; time = 0.0382s; samplesPerSecond = 33481.6
08/16/2016 10:01:39:  Epoch[38 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.14715699 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0447s; samplesPerSecond = 28645.6
08/16/2016 10:01:39:  Epoch[38 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.14940417 * 1280; EvalErrorPrediction = 0.06718750 * 1280; time = 0.0728s; samplesPerSecond = 17577.6
08/16/2016 10:01:39:  Epoch[38 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16454649 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0489s; samplesPerSecond = 26193.5
08/16/2016 10:01:39:  Epoch[38 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14210191 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0394s; samplesPerSecond = 32484.8
08/16/2016 10:01:39:  Epoch[38 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16033564 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0438s; samplesPerSecond = 29216.4
08/16/2016 10:01:39:  Epoch[38 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.14252672 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0977s; samplesPerSecond = 13103.7
08/16/2016 10:01:40: Finished Epoch[38 of 50]: [Training] CrossEntropyWithSoftmax = 0.15447957 * 10000; EvalErrorPrediction = 0.07520000 * 10000; totalSamplesSeen = 380000; learningRatePerSample = 0.1; epochTime=0.526384s
08/16/2016 10:01:40: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.38'

08/16/2016 10:01:40: Starting Epoch 39: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 38: frames [380000..390000] (first sequence at sample 380000), data subset 0 of 1

08/16/2016 10:01:40: Starting minibatch loop.
08/16/2016 10:01:40:  Epoch[39 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15245637 * 1280; EvalErrorPrediction = 0.07890625 * 1280; time = 0.0513s; samplesPerSecond = 24962.9
08/16/2016 10:01:40:  Epoch[39 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.14150614 * 1280; EvalErrorPrediction = 0.06328125 * 1280; time = 0.0435s; samplesPerSecond = 29439.5
08/16/2016 10:01:40:  Epoch[39 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15755613 * 1280; EvalErrorPrediction = 0.07890625 * 1280; time = 0.0673s; samplesPerSecond = 19016.5
08/16/2016 10:01:40:  Epoch[39 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15467634 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.1220s; samplesPerSecond = 10494.8
08/16/2016 10:01:40:  Epoch[39 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15592108 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0383s; samplesPerSecond = 33460.6
08/16/2016 10:01:40:  Epoch[39 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.17342644 * 1280; EvalErrorPrediction = 0.08437500 * 1280; time = 0.0421s; samplesPerSecond = 30393.7
08/16/2016 10:01:40:  Epoch[39 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.17978611 * 1280; EvalErrorPrediction = 0.09375000 * 1280; time = 0.0378s; samplesPerSecond = 33832.0
08/16/2016 10:01:40: Finished Epoch[39 of 50]: [Training] CrossEntropyWithSoftmax = 0.15823716 * 10000; EvalErrorPrediction = 0.07810000 * 10000; totalSamplesSeen = 390000; learningRatePerSample = 0.1; epochTime=0.451704s
08/16/2016 10:01:40: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.39'

08/16/2016 10:01:40: Starting Epoch 40: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 39: frames [390000..400000] (first sequence at sample 390000), data subset 0 of 1

08/16/2016 10:01:40: Starting minibatch loop.
08/16/2016 10:01:40:  Epoch[40 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.18975155 * 1280; EvalErrorPrediction = 0.09140625 * 1280; time = 0.0366s; samplesPerSecond = 34970.8
08/16/2016 10:01:40:  Epoch[40 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.14647381 * 1280; EvalErrorPrediction = 0.06718750 * 1280; time = 0.0703s; samplesPerSecond = 18215.7
08/16/2016 10:01:40:  Epoch[40 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15599771 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0365s; samplesPerSecond = 35041.6
08/16/2016 10:01:40:  Epoch[40 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15626054 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0431s; samplesPerSecond = 29681.8
08/16/2016 10:01:40:  Epoch[40 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15985618 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0443s; samplesPerSecond = 28882.8
08/16/2016 10:01:40:  Epoch[40 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16492434 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0484s; samplesPerSecond = 26459.9
08/16/2016 10:01:40:  Epoch[40 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.14662266 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0479s; samplesPerSecond = 26749.7
08/16/2016 10:01:40: Finished Epoch[40 of 50]: [Training] CrossEntropyWithSoftmax = 0.15780449 * 10000; EvalErrorPrediction = 0.07550000 * 10000; totalSamplesSeen = 400000; learningRatePerSample = 0.1; epochTime=0.380356s
08/16/2016 10:01:40: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.40'

08/16/2016 10:01:40: Starting Epoch 41: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 40: frames [400000..410000] (first sequence at sample 400000), data subset 0 of 1

08/16/2016 10:01:40: Starting minibatch loop.
08/16/2016 10:01:40:  Epoch[41 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16937305 * 1280; EvalErrorPrediction = 0.08437500 * 1280; time = 0.0543s; samplesPerSecond = 23568.8
08/16/2016 10:01:40:  Epoch[41 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.13820328 * 1280; EvalErrorPrediction = 0.06093750 * 1280; time = 0.0419s; samplesPerSecond = 30519.1
08/16/2016 10:01:41:  Epoch[41 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15636744 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0461s; samplesPerSecond = 27793.5
08/16/2016 10:01:41:  Epoch[41 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15901361 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0356s; samplesPerSecond = 35922.8
08/16/2016 10:01:41:  Epoch[41 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15166650 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0501s; samplesPerSecond = 25536.7
08/16/2016 10:01:41:  Epoch[41 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15241671 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0542s; samplesPerSecond = 23619.7
08/16/2016 10:01:41:  Epoch[41 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15653992 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0433s; samplesPerSecond = 29583.7
08/16/2016 10:01:41: Finished Epoch[41 of 50]: [Training] CrossEntropyWithSoftmax = 0.15444556 * 10000; EvalErrorPrediction = 0.07310000 * 10000; totalSamplesSeen = 410000; learningRatePerSample = 0.1; epochTime=0.375029s
08/16/2016 10:01:41: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.41'

08/16/2016 10:01:41: Starting Epoch 42: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 41: frames [410000..420000] (first sequence at sample 410000), data subset 0 of 1

08/16/2016 10:01:41: Starting minibatch loop.
08/16/2016 10:01:41:  Epoch[42 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15034840 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0349s; samplesPerSecond = 36710.9
08/16/2016 10:01:41:  Epoch[42 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.14989798 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0711s; samplesPerSecond = 18002.8
08/16/2016 10:01:41:  Epoch[42 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15917435 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0384s; samplesPerSecond = 33332.5
08/16/2016 10:01:41:  Epoch[42 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15578203 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0989s; samplesPerSecond = 12940.0
08/16/2016 10:01:41:  Epoch[42 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16280818 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0412s; samplesPerSecond = 31098.2
08/16/2016 10:01:41:  Epoch[42 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16846323 * 1280; EvalErrorPrediction = 0.08046875 * 1280; time = 0.0401s; samplesPerSecond = 31903.5
08/16/2016 10:01:41:  Epoch[42 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.17319469 * 1280; EvalErrorPrediction = 0.08437500 * 1280; time = 0.0432s; samplesPerSecond = 29618.0
08/16/2016 10:01:41: Finished Epoch[42 of 50]: [Training] CrossEntropyWithSoftmax = 0.15898705 * 10000; EvalErrorPrediction = 0.07500000 * 10000; totalSamplesSeen = 420000; learningRatePerSample = 0.1; epochTime=0.408516s
08/16/2016 10:01:41: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.42'

08/16/2016 10:01:41: Starting Epoch 43: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 42: frames [420000..430000] (first sequence at sample 420000), data subset 0 of 1

08/16/2016 10:01:41: Starting minibatch loop.
08/16/2016 10:01:41:  Epoch[43 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16630217 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0450s; samplesPerSecond = 28459.6
08/16/2016 10:01:41:  Epoch[43 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.13294576 * 1280; EvalErrorPrediction = 0.06250000 * 1280; time = 0.0460s; samplesPerSecond = 27845.5
08/16/2016 10:01:41:  Epoch[43 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15917583 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0613s; samplesPerSecond = 20869.3
08/16/2016 10:01:41:  Epoch[43 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15790648 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0456s; samplesPerSecond = 28061.6
08/16/2016 10:01:41:  Epoch[43 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16472111 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0494s; samplesPerSecond = 25929.3
08/16/2016 10:01:41:  Epoch[43 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.14034090 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0415s; samplesPerSecond = 30821.8
08/16/2016 10:01:42:  Epoch[43 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16956015 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0438s; samplesPerSecond = 29255.1
08/16/2016 10:01:42: Finished Epoch[43 of 50]: [Training] CrossEntropyWithSoftmax = 0.15555933 * 10000; EvalErrorPrediction = 0.07620000 * 10000; totalSamplesSeen = 430000; learningRatePerSample = 0.1; epochTime=0.382102s
08/16/2016 10:01:42: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.43'

08/16/2016 10:01:42: Starting Epoch 44: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 43: frames [430000..440000] (first sequence at sample 430000), data subset 0 of 1

08/16/2016 10:01:42: Starting minibatch loop.
08/16/2016 10:01:42:  Epoch[44 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16926413 * 1280; EvalErrorPrediction = 0.08437500 * 1280; time = 0.0406s; samplesPerSecond = 31497.6
08/16/2016 10:01:42:  Epoch[44 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16363897 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0504s; samplesPerSecond = 25409.9
08/16/2016 10:01:42:  Epoch[44 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.13639555 * 1280; EvalErrorPrediction = 0.06640625 * 1280; time = 0.0557s; samplesPerSecond = 22976.5
08/16/2016 10:01:42:  Epoch[44 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16664805 * 1280; EvalErrorPrediction = 0.08593750 * 1280; time = 0.0459s; samplesPerSecond = 27905.6
08/16/2016 10:01:42:  Epoch[44 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16281080 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0413s; samplesPerSecond = 31024.3
08/16/2016 10:01:42:  Epoch[44 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.13564296 * 1280; EvalErrorPrediction = 0.06484375 * 1280; time = 0.0443s; samplesPerSecond = 28921.3
08/16/2016 10:01:42:  Epoch[44 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16526623 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.1390s; samplesPerSecond = 9207.6
08/16/2016 10:01:42: Finished Epoch[44 of 50]: [Training] CrossEntropyWithSoftmax = 0.15709537 * 10000; EvalErrorPrediction = 0.07680000 * 10000; totalSamplesSeen = 440000; learningRatePerSample = 0.1; epochTime=0.449795s
08/16/2016 10:01:42: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.44'

08/16/2016 10:01:42: Starting Epoch 45: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 44: frames [440000..450000] (first sequence at sample 440000), data subset 0 of 1

08/16/2016 10:01:42: Starting minibatch loop.
08/16/2016 10:01:42:  Epoch[45 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16758128 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0492s; samplesPerSecond = 26027.9
08/16/2016 10:01:42:  Epoch[45 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.14725612 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0358s; samplesPerSecond = 35729.2
08/16/2016 10:01:42:  Epoch[45 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16129522 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0374s; samplesPerSecond = 34230.1
08/16/2016 10:01:42:  Epoch[45 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16227765 * 1280; EvalErrorPrediction = 0.08437500 * 1280; time = 0.0406s; samplesPerSecond = 31527.9
08/16/2016 10:01:42:  Epoch[45 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15812397 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0453s; samplesPerSecond = 28244.2
08/16/2016 10:01:42:  Epoch[45 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.14453859 * 1280; EvalErrorPrediction = 0.06718750 * 1280; time = 0.0674s; samplesPerSecond = 18993.9
08/16/2016 10:01:42:  Epoch[45 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15846949 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0668s; samplesPerSecond = 19173.7
08/16/2016 10:01:42: Finished Epoch[45 of 50]: [Training] CrossEntropyWithSoftmax = 0.15527948 * 10000; EvalErrorPrediction = 0.07570000 * 10000; totalSamplesSeen = 450000; learningRatePerSample = 0.1; epochTime=0.384077s
08/16/2016 10:01:42: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.45'

08/16/2016 10:01:42: Starting Epoch 46: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 45: frames [450000..460000] (first sequence at sample 450000), data subset 0 of 1

08/16/2016 10:01:42: Starting minibatch loop.
08/16/2016 10:01:42:  Epoch[46 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.13163276 * 1280; EvalErrorPrediction = 0.05625000 * 1280; time = 0.0448s; samplesPerSecond = 28560.0
08/16/2016 10:01:43:  Epoch[46 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17128520 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0504s; samplesPerSecond = 25409.4
08/16/2016 10:01:43:  Epoch[46 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15425162 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0620s; samplesPerSecond = 20655.5
08/16/2016 10:01:43:  Epoch[46 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15192242 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0582s; samplesPerSecond = 21980.3
08/16/2016 10:01:43:  Epoch[46 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16164570 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0838s; samplesPerSecond = 15281.0
08/16/2016 10:01:43:  Epoch[46 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16738505 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0446s; samplesPerSecond = 28691.2
08/16/2016 10:01:43:  Epoch[46 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16348038 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0460s; samplesPerSecond = 27800.1
08/16/2016 10:01:43: Finished Epoch[46 of 50]: [Training] CrossEntropyWithSoftmax = 0.15742201 * 10000; EvalErrorPrediction = 0.07540000 * 10000; totalSamplesSeen = 460000; learningRatePerSample = 0.1; epochTime=0.489905s
08/16/2016 10:01:43: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.46'

08/16/2016 10:01:43: Starting Epoch 47: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 46: frames [460000..470000] (first sequence at sample 460000), data subset 0 of 1

08/16/2016 10:01:43: Starting minibatch loop.
08/16/2016 10:01:43:  Epoch[47 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16164525 * 1280; EvalErrorPrediction = 0.08046875 * 1280; time = 0.0392s; samplesPerSecond = 32651.4
08/16/2016 10:01:43:  Epoch[47 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16455059 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.1163s; samplesPerSecond = 11006.0
08/16/2016 10:01:43:  Epoch[47 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16870224 * 1280; EvalErrorPrediction = 0.09140625 * 1280; time = 0.0392s; samplesPerSecond = 32661.4
08/16/2016 10:01:43:  Epoch[47 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.12883611 * 1280; EvalErrorPrediction = 0.06328125 * 1280; time = 0.0441s; samplesPerSecond = 29007.8
08/16/2016 10:01:43:  Epoch[47 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14594216 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0436s; samplesPerSecond = 29347.0
08/16/2016 10:01:43:  Epoch[47 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15981717 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0403s; samplesPerSecond = 31744.5
08/16/2016 10:01:43:  Epoch[47 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.14501266 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0414s; samplesPerSecond = 30882.8
08/16/2016 10:01:43: Finished Epoch[47 of 50]: [Training] CrossEntropyWithSoftmax = 0.15290601 * 10000; EvalErrorPrediction = 0.07500000 * 10000; totalSamplesSeen = 470000; learningRatePerSample = 0.1; epochTime=0.408889s
08/16/2016 10:01:43: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.47'

08/16/2016 10:01:43: Starting Epoch 48: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 47: frames [470000..480000] (first sequence at sample 470000), data subset 0 of 1

08/16/2016 10:01:43: Starting minibatch loop.
08/16/2016 10:01:43:  Epoch[48 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16257966 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0446s; samplesPerSecond = 28710.5
08/16/2016 10:01:43:  Epoch[48 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16322243 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.1229s; samplesPerSecond = 10417.9
08/16/2016 10:01:44:  Epoch[48 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16032901 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0382s; samplesPerSecond = 33486.8
08/16/2016 10:01:44:  Epoch[48 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15169969 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0447s; samplesPerSecond = 28655.2
08/16/2016 10:01:44:  Epoch[48 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15752759 * 1280; EvalErrorPrediction = 0.08046875 * 1280; time = 0.0452s; samplesPerSecond = 28306.1
08/16/2016 10:01:44:  Epoch[48 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.14941444 * 1280; EvalErrorPrediction = 0.06796875 * 1280; time = 0.0456s; samplesPerSecond = 28051.7
08/16/2016 10:01:44:  Epoch[48 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.14432421 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0615s; samplesPerSecond = 20813.0
08/16/2016 10:01:44: Finished Epoch[48 of 50]: [Training] CrossEntropyWithSoftmax = 0.15491803 * 10000; EvalErrorPrediction = 0.07620000 * 10000; totalSamplesSeen = 480000; learningRatePerSample = 0.1; epochTime=0.460735s
08/16/2016 10:01:44: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.48'

08/16/2016 10:01:44: Starting Epoch 49: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 48: frames [480000..490000] (first sequence at sample 480000), data subset 0 of 1

08/16/2016 10:01:44: Starting minibatch loop.
08/16/2016 10:01:44:  Epoch[49 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16991386 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0448s; samplesPerSecond = 28552.3
08/16/2016 10:01:44:  Epoch[49 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15043976 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0658s; samplesPerSecond = 19441.4
08/16/2016 10:01:44:  Epoch[49 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16458762 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0483s; samplesPerSecond = 26524.6
08/16/2016 10:01:44:  Epoch[49 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15567193 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0709s; samplesPerSecond = 18058.9
08/16/2016 10:01:44:  Epoch[49 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14840393 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0642s; samplesPerSecond = 19949.2
08/16/2016 10:01:44:  Epoch[49 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15142746 * 1280; EvalErrorPrediction = 0.06718750 * 1280; time = 0.0444s; samplesPerSecond = 28798.3
08/16/2016 10:01:44:  Epoch[49 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.19236517 * 1280; EvalErrorPrediction = 0.08984375 * 1280; time = 0.0453s; samplesPerSecond = 28239.9
08/16/2016 10:01:44: Finished Epoch[49 of 50]: [Training] CrossEntropyWithSoftmax = 0.16079432 * 10000; EvalErrorPrediction = 0.07610000 * 10000; totalSamplesSeen = 490000; learningRatePerSample = 0.1; epochTime=0.449659s
08/16/2016 10:01:44: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.49'

08/16/2016 10:01:44: Starting Epoch 50: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 49: frames [490000..500000] (first sequence at sample 490000), data subset 0 of 1

08/16/2016 10:01:44: Starting minibatch loop.
08/16/2016 10:01:44:  Epoch[50 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15400987 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0457s; samplesPerSecond = 27995.9
08/16/2016 10:01:44:  Epoch[50 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.14237220 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0416s; samplesPerSecond = 30741.1
08/16/2016 10:01:44:  Epoch[50 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15252087 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.1358s; samplesPerSecond = 9428.1
08/16/2016 10:01:44:  Epoch[50 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16386962 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0407s; samplesPerSecond = 31476.7
08/16/2016 10:01:45:  Epoch[50 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14503121 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0642s; samplesPerSecond = 19937.4
08/16/2016 10:01:45:  Epoch[50 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16062884 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0442s; samplesPerSecond = 28932.4
08/16/2016 10:01:45:  Epoch[50 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15499449 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0535s; samplesPerSecond = 23937.8
08/16/2016 10:01:45: Finished Epoch[50 of 50]: [Training] CrossEntropyWithSoftmax = 0.15345076 * 10000; EvalErrorPrediction = 0.07620000 * 10000; totalSamplesSeen = 500000; learningRatePerSample = 0.1; epochTime=0.49265s
08/16/2016 10:01:45: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn'
08/16/2016 10:01:45: CNTKCommandTrainEnd: Simple_Demo

08/16/2016 10:01:45: Action "train" complete.


08/16/2016 10:01:45: ##############################################################################
08/16/2016 10:01:45: #                                                                            #
08/16/2016 10:01:45: # Action "write"                                                             #
08/16/2016 10:01:45: #                                                                            #
08/16/2016 10:01:45: ##############################################################################


Post-processing network...

7 roots:
	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax()
	EvalErrorPrediction = ErrorPrediction()
	InvStdOfFeatures = InvStdDev()
	MeanOfFeatures = Mean()
	PosteriorProb = Softmax()
	Prior = Mean()
	ScaledLogLikelihood = Minus()

Validating network. 25 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [2 x *1]
Validating --> W2 = LearnableParameter() :  -> [2 x 50]
Validating --> W1 = LearnableParameter() :  -> [50 x 50]
Validating --> W0 = LearnableParameter() :  -> [50 x 2]
Validating --> features = InputValue() :  -> [2 x *1]
Validating --> MeanOfFeatures = Mean (features) : [2 x *1] -> [2]
Validating --> InvStdOfFeatures = InvStdDev (features) : [2 x *1] -> [2]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization (features, MeanOfFeatures, InvStdOfFeatures) : [2 x *1], [2], [2] -> [2 x *1]
Validating --> W0*features = Times (W0, MVNormalizedFeatures) : [50 x 2], [2 x *1] -> [50 x *1]
Validating --> B0 = LearnableParameter() :  -> [50 x 1]
Validating --> W0*features+B0 = Plus (W0*features, B0) : [50 x *1], [50 x 1] -> [50 x 1 x *1]
Validating --> H1 = Sigmoid (W0*features+B0) : [50 x 1 x *1] -> [50 x 1 x *1]
Validating --> W1*H1 = Times (W1, H1) : [50 x 50], [50 x 1 x *1] -> [50 x 1 x *1]
Validating --> B1 = LearnableParameter() :  -> [50 x 1]
Validating --> W1*H1+B1 = Plus (W1*H1, B1) : [50 x 1 x *1], [50 x 1] -> [50 x 1 x *1]
Validating --> H2 = Sigmoid (W1*H1+B1) : [50 x 1 x *1] -> [50 x 1 x *1]
Validating --> W2*H1 = Times (W2, H2) : [2 x 50], [50 x 1 x *1] -> [2 x 1 x *1]
Validating --> B2 = LearnableParameter() :  -> [2 x 1]
Validating --> HLast = Plus (W2*H1, B2) : [2 x 1 x *1], [2 x 1] -> [2 x 1 x *1]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax (labels, HLast) : [2 x *1], [2 x 1 x *1] -> [1]
Validating --> EvalErrorPrediction = ErrorPrediction (labels, HLast) : [2 x *1], [2 x 1 x *1] -> [1]
Validating --> PosteriorProb = Softmax (HLast) : [2 x 1 x *1] -> [2 x 1 x *1]
Validating --> Prior = Mean (labels) : [2 x *1] -> [2]
Validating --> LogOfPrior = Log (Prior) : [2] -> [2]
Validating --> ScaledLogLikelihood = Minus (HLast, LogOfPrior) : [2 x 1 x *1], [2] -> [2 x 1 x *1]

Validating network. 17 nodes to process in pass 2.


Validating network, final pass.



12 out of 25 nodes do not share the minibatch layout with the input data.

Post-processing network complete.



Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 25 matrices, 3 are shared as 1, and 22 are not shared.

	{ CrossEntropyWithSoftmax : [1]
	  EvalErrorPrediction : [1]
	  PosteriorProb : [2 x 1 x *1] }

Minibatch[0]: ActualMBSize = 603
Written to /tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/SimpleOutput*
Total Samples Evaluated = 603

08/16/2016 10:01:45: Action "write" complete.

08/16/2016 10:01:45: __COMPLETED__
=== Deleting last epoch data
==== Re-running from checkpoint
=== Running /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple/cntk.cntk currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data RunDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple OutputDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu DeviceId=-1 timestamping=true makeMode=true
-------------------------------------------------------------------
Build info: 

		Built time: Aug 16 2016 09:41:56
		Last modified date: Fri Aug 12 07:32:43 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 026b1e772b963461e189f8f00aa7ed6951298f84
		Built by philly on f67b30a647de
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
08/16/2016 10:01:45: -------------------------------------------------------------------
08/16/2016 10:01:45: Build info: 

08/16/2016 10:01:45: 		Built time: Aug 16 2016 09:41:56
08/16/2016 10:01:45: 		Last modified date: Fri Aug 12 07:32:43 2016
08/16/2016 10:01:45: 		Build type: release
08/16/2016 10:01:45: 		Build target: GPU
08/16/2016 10:01:45: 		With 1bit-SGD: no
08/16/2016 10:01:45: 		Math lib: mkl
08/16/2016 10:01:45: 		CUDA_PATH: /usr/local/cuda-7.5
08/16/2016 10:01:45: 		CUB_PATH: /usr/local/cub-1.4.1
08/16/2016 10:01:45: 		CUDNN_PATH: /usr/local/cudnn-4.0
08/16/2016 10:01:45: 		Build Branch: HEAD
08/16/2016 10:01:45: 		Build SHA1: 026b1e772b963461e189f8f00aa7ed6951298f84
08/16/2016 10:01:45: 		Built by philly on f67b30a647de
08/16/2016 10:01:45: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
08/16/2016 10:01:45: -------------------------------------------------------------------
08/16/2016 10:01:46: -------------------------------------------------------------------
08/16/2016 10:01:46: GPU info:

08/16/2016 10:01:46: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/16/2016 10:01:46: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/16/2016 10:01:46: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/16/2016 10:01:46: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/16/2016 10:01:46: -------------------------------------------------------------------

08/16/2016 10:01:46: Running on localhost at 2016/08/16 10:01:46
08/16/2016 10:01:46: Command line: 
/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple/cntk.cntk  currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  RunDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu  DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple  OutputDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu  DeviceId=-1  timestamping=true  makeMode=true



08/16/2016 10:01:46: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
08/16/2016 10:01:46: command=Simple_Demo:Simple_Demo_Output
DeviceNumber=-1
precision=float
modelPath=$RunDir$/models/simple.dnn
deviceId=$DeviceNumber$
outputNodeNames=ScaledLogLikelihood
traceLevel=1
Simple_Demo=[
    action=train
    SimpleNetworkBuilder=[
        layerSizes=2:50*2:2
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        initValueScale=1.0
        applyMeanVarNorm=true
        uniformInit=true
        needPrior=true
    ]
    SGD=[
        epochSize=0 
        minibatchSize=128
        learningRatesPerSample=0.1
        momentumAsTimeConstant=2500
        dropoutRate=0.0
        maxEpochs=50
        keepCheckPointFiles = true
    ]
    reader=[
        readerType=CNTKTextFormatReader
        file=$DataDir$/SimpleDataTrain_cntk_text.txt
        input = [
            features=[
dim = 2      
                format = "dense"
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
]
Simple_Demo_Output=[
    action=write
    reader=[
        readerType=CNTKTextFormatReader
        file=$DataDir$/SimpleDataTest_cntk_text.txt
        input = [
            features=[
dim = 2 
                format = "dense" 
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
outputPath=$RunDir$/SimpleOutput    
]
currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
RunDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu
DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple
OutputDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu
DeviceId=-1
timestamping=true
makeMode=true

08/16/2016 10:01:46: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

08/16/2016 10:01:46: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
08/16/2016 10:01:46: command=Simple_Demo:Simple_Demo_Output
DeviceNumber=-1
precision=float
modelPath=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn
deviceId=-1
outputNodeNames=ScaledLogLikelihood
traceLevel=1
Simple_Demo=[
    action=train
    SimpleNetworkBuilder=[
        layerSizes=2:50*2:2
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        initValueScale=1.0
        applyMeanVarNorm=true
        uniformInit=true
        needPrior=true
    ]
    SGD=[
        epochSize=0 
        minibatchSize=128
        learningRatesPerSample=0.1
        momentumAsTimeConstant=2500
        dropoutRate=0.0
        maxEpochs=50
        keepCheckPointFiles = true
    ]
    reader=[
        readerType=CNTKTextFormatReader
        file=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/SimpleDataTrain_cntk_text.txt
        input = [
            features=[
dim = 2      
                format = "dense"
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
]
Simple_Demo_Output=[
    action=write
    reader=[
        readerType=CNTKTextFormatReader
        file=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/SimpleDataTest_cntk_text.txt
        input = [
            features=[
dim = 2 
                format = "dense" 
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
outputPath=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/SimpleOutput    
]
currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
RunDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu
DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple
OutputDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu
DeviceId=-1
timestamping=true
makeMode=true

08/16/2016 10:01:46: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

08/16/2016 10:01:46: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk.cntk:command=Simple_Demo:Simple_Demo_Output
configparameters: cntk.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple
configparameters: cntk.cntk:currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
configparameters: cntk.cntk:DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
configparameters: cntk.cntk:deviceId=-1
configparameters: cntk.cntk:DeviceNumber=-1
configparameters: cntk.cntk:makeMode=true
configparameters: cntk.cntk:modelPath=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn
configparameters: cntk.cntk:OutputDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu
configparameters: cntk.cntk:outputNodeNames=ScaledLogLikelihood
configparameters: cntk.cntk:precision=float
configparameters: cntk.cntk:RunDir=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu
configparameters: cntk.cntk:Simple_Demo=[
    action=train
    SimpleNetworkBuilder=[
        layerSizes=2:50*2:2
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        initValueScale=1.0
        applyMeanVarNorm=true
        uniformInit=true
        needPrior=true
    ]
    SGD=[
        epochSize=0 
        minibatchSize=128
        learningRatesPerSample=0.1
        momentumAsTimeConstant=2500
        dropoutRate=0.0
        maxEpochs=50
        keepCheckPointFiles = true
    ]
    reader=[
        readerType=CNTKTextFormatReader
        file=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/SimpleDataTrain_cntk_text.txt
        input = [
            features=[
dim = 2      
                format = "dense"
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
]

configparameters: cntk.cntk:Simple_Demo_Output=[
    action=write
    reader=[
        readerType=CNTKTextFormatReader
        file=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/SimpleDataTest_cntk_text.txt
        input = [
            features=[
dim = 2 
                format = "dense" 
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
outputPath=/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/SimpleOutput    
]

configparameters: cntk.cntk:timestamping=true
configparameters: cntk.cntk:traceLevel=1
08/16/2016 10:01:46: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
08/16/2016 10:01:46: Commands: Simple_Demo Simple_Demo_Output
08/16/2016 10:01:46: Precision = "float"
08/16/2016 10:01:46: CNTKModelPath: /tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn
08/16/2016 10:01:46: CNTKCommandTrainInfo: Simple_Demo : 50
08/16/2016 10:01:46: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 50

08/16/2016 10:01:46: ##############################################################################
08/16/2016 10:01:46: #                                                                            #
08/16/2016 10:01:46: # Action "train"                                                             #
08/16/2016 10:01:46: #                                                                            #
08/16/2016 10:01:46: ##############################################################################

08/16/2016 10:01:46: CNTKCommandTrainBegin: Simple_Demo
SimpleNetworkBuilder Using CPU

08/16/2016 10:01:46: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn.49'.

Post-processing network...

7 roots:
	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax()
	EvalErrorPrediction = ErrorPrediction()
	InvStdOfFeatures = InvStdDev()
	MeanOfFeatures = Mean()
	PosteriorProb = Softmax()
	Prior = Mean()
	ScaledLogLikelihood = Minus()

Validating network. 25 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [2 x *1]
Validating --> W2 = LearnableParameter() :  -> [2 x 50]
Validating --> W1 = LearnableParameter() :  -> [50 x 50]
Validating --> W0 = LearnableParameter() :  -> [50 x 2]
Validating --> features = InputValue() :  -> [2 x *1]
Validating --> MeanOfFeatures = Mean (features) : [2 x *1] -> [2]
Validating --> InvStdOfFeatures = InvStdDev (features) : [2 x *1] -> [2]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization (features, MeanOfFeatures, InvStdOfFeatures) : [2 x *1], [2], [2] -> [2 x *1]
Validating --> W0*features = Times (W0, MVNormalizedFeatures) : [50 x 2], [2 x *1] -> [50 x *1]
Validating --> B0 = LearnableParameter() :  -> [50 x 1]
Validating --> W0*features+B0 = Plus (W0*features, B0) : [50 x *1], [50 x 1] -> [50 x 1 x *1]
Validating --> H1 = Sigmoid (W0*features+B0) : [50 x 1 x *1] -> [50 x 1 x *1]
Validating --> W1*H1 = Times (W1, H1) : [50 x 50], [50 x 1 x *1] -> [50 x 1 x *1]
Validating --> B1 = LearnableParameter() :  -> [50 x 1]
Validating --> W1*H1+B1 = Plus (W1*H1, B1) : [50 x 1 x *1], [50 x 1] -> [50 x 1 x *1]
Validating --> H2 = Sigmoid (W1*H1+B1) : [50 x 1 x *1] -> [50 x 1 x *1]
Validating --> W2*H1 = Times (W2, H2) : [2 x 50], [50 x 1 x *1] -> [2 x 1 x *1]
Validating --> B2 = LearnableParameter() :  -> [2 x 1]
Validating --> HLast = Plus (W2*H1, B2) : [2 x 1 x *1], [2 x 1] -> [2 x 1 x *1]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax (labels, HLast) : [2 x *1], [2 x 1 x *1] -> [1]
Validating --> EvalErrorPrediction = ErrorPrediction (labels, HLast) : [2 x *1], [2 x 1 x *1] -> [1]
Validating --> PosteriorProb = Softmax (HLast) : [2 x 1 x *1] -> [2 x 1 x *1]
Validating --> Prior = Mean (labels) : [2 x *1] -> [2]
Validating --> LogOfPrior = Log (Prior) : [2] -> [2]
Validating --> ScaledLogLikelihood = Minus (HLast, LogOfPrior) : [2 x 1 x *1], [2] -> [2 x 1 x *1]

Validating network. 17 nodes to process in pass 2.


Validating network, final pass.



12 out of 25 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/16/2016 10:01:46: Loaded model with 25 nodes on CPU.

08/16/2016 10:01:46: Training criterion node(s):
08/16/2016 10:01:46: 	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax

08/16/2016 10:01:46: Evaluation criterion node(s):
08/16/2016 10:01:46: 	EvalErrorPrediction = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 40 matrices, 19 are shared as 8, and 21 are not shared.

	{ W0 : [50 x 2] (gradient)
	  W0*features+B0 : [50 x 1 x *1] }
	{ H1 : [50 x 1 x *1]
	  W0*features : [50 x *1] (gradient) }
	{ W0*features+B0 : [50 x 1 x *1] (gradient)
	  W1*H1 : [50 x 1 x *1] }
	{ W1 : [50 x 50] (gradient)
	  W1*H1+B1 : [50 x 1 x *1] }
	{ H2 : [50 x 1 x *1]
	  W1*H1 : [50 x 1 x *1] (gradient) }
	{ B0 : [50 x 1] (gradient)
	  H1 : [50 x 1 x *1] (gradient)
	  W1*H1+B1 : [50 x 1 x *1] (gradient)
	  W2*H1 : [2 x 1 x *1] }
	{ HLast : [2 x 1 x *1]
	  W2 : [2 x 50] (gradient) }
	{ B1 : [50 x 1] (gradient)
	  H2 : [50 x 1 x *1] (gradient)
	  HLast : [2 x 1 x *1] (gradient) }


08/16/2016 10:01:46: Training 2802 parameters in 6 out of 6 parameter tensors and 15 nodes with gradient:

08/16/2016 10:01:46: 	Node 'B0' (LearnableParameter operation) : [50 x 1]
08/16/2016 10:01:46: 	Node 'B1' (LearnableParameter operation) : [50 x 1]
08/16/2016 10:01:46: 	Node 'B2' (LearnableParameter operation) : [2 x 1]
08/16/2016 10:01:46: 	Node 'W0' (LearnableParameter operation) : [50 x 2]
08/16/2016 10:01:46: 	Node 'W1' (LearnableParameter operation) : [50 x 50]
08/16/2016 10:01:46: 	Node 'W2' (LearnableParameter operation) : [2 x 50]

08/16/2016 10:01:46: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

08/16/2016 10:01:46: Starting Epoch 50: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 49: frames [490000..500000] (first sequence at sample 490000), data subset 0 of 1

08/16/2016 10:01:46: Starting minibatch loop.
08/16/2016 10:01:46:  Epoch[50 of 50]-Minibatch[   1-  10, 0.00000000000003%]: CrossEntropyWithSoftmax = 0.15400987 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.2264s; samplesPerSecond = 5654.1
08/16/2016 10:01:46:  Epoch[50 of 50]-Minibatch[  11-  20, 0.00000000000006%]: CrossEntropyWithSoftmax = 0.14237220 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0450s; samplesPerSecond = 28448.2
08/16/2016 10:01:46:  Epoch[50 of 50]-Minibatch[  21-  30, 0.00000000000008%]: CrossEntropyWithSoftmax = 0.15252087 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0387s; samplesPerSecond = 33069.8
08/16/2016 10:01:46:  Epoch[50 of 50]-Minibatch[  31-  40, 0.00000000000011%]: CrossEntropyWithSoftmax = 0.16386962 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0359s; samplesPerSecond = 35652.6
08/16/2016 10:01:46:  Epoch[50 of 50]-Minibatch[  41-  50, 0.00000000000014%]: CrossEntropyWithSoftmax = 0.14503121 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0500s; samplesPerSecond = 25598.0
08/16/2016 10:01:46:  Epoch[50 of 50]-Minibatch[  51-  60, 0.00000000000017%]: CrossEntropyWithSoftmax = 0.16062884 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0437s; samplesPerSecond = 29320.1
08/16/2016 10:01:47:  Epoch[50 of 50]-Minibatch[  61-  70, 0.00000000000019%]: CrossEntropyWithSoftmax = 0.15499449 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0439s; samplesPerSecond = 29145.2
08/16/2016 10:01:47: Finished Epoch[50 of 50]: [Training] CrossEntropyWithSoftmax = 0.15345076 * 10000; EvalErrorPrediction = 0.07620000 * 10000; totalSamplesSeen = 500000; learningRatePerSample = 0.1; epochTime=0.527256s
08/16/2016 10:01:47: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/models/simple.dnn'
08/16/2016 10:01:47: CNTKCommandTrainEnd: Simple_Demo

08/16/2016 10:01:47: Action "train" complete.


08/16/2016 10:01:47: ##############################################################################
08/16/2016 10:01:47: #                                                                            #
08/16/2016 10:01:47: # Action "write"                                                             #
08/16/2016 10:01:47: #                                                                            #
08/16/2016 10:01:47: ##############################################################################


Post-processing network...

7 roots:
	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax()
	EvalErrorPrediction = ErrorPrediction()
	InvStdOfFeatures = InvStdDev()
	MeanOfFeatures = Mean()
	PosteriorProb = Softmax()
	Prior = Mean()
	ScaledLogLikelihood = Minus()

Validating network. 25 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [2 x *2]
Validating --> W2 = LearnableParameter() :  -> [2 x 50]
Validating --> W1 = LearnableParameter() :  -> [50 x 50]
Validating --> W0 = LearnableParameter() :  -> [50 x 2]
Validating --> features = InputValue() :  -> [2 x *2]
Validating --> MeanOfFeatures = Mean (features) : [2 x *2] -> [2]
Validating --> InvStdOfFeatures = InvStdDev (features) : [2 x *2] -> [2]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization (features, MeanOfFeatures, InvStdOfFeatures) : [2 x *2], [2], [2] -> [2 x *2]
Validating --> W0*features = Times (W0, MVNormalizedFeatures) : [50 x 2], [2 x *2] -> [50 x *2]
Validating --> B0 = LearnableParameter() :  -> [50 x 1]
Validating --> W0*features+B0 = Plus (W0*features, B0) : [50 x *2], [50 x 1] -> [50 x 1 x *2]
Validating --> H1 = Sigmoid (W0*features+B0) : [50 x 1 x *2] -> [50 x 1 x *2]
Validating --> W1*H1 = Times (W1, H1) : [50 x 50], [50 x 1 x *2] -> [50 x 1 x *2]
Validating --> B1 = LearnableParameter() :  -> [50 x 1]
Validating --> W1*H1+B1 = Plus (W1*H1, B1) : [50 x 1 x *2], [50 x 1] -> [50 x 1 x *2]
Validating --> H2 = Sigmoid (W1*H1+B1) : [50 x 1 x *2] -> [50 x 1 x *2]
Validating --> W2*H1 = Times (W2, H2) : [2 x 50], [50 x 1 x *2] -> [2 x 1 x *2]
Validating --> B2 = LearnableParameter() :  -> [2 x 1]
Validating --> HLast = Plus (W2*H1, B2) : [2 x 1 x *2], [2 x 1] -> [2 x 1 x *2]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax (labels, HLast) : [2 x *2], [2 x 1 x *2] -> [1]
Validating --> EvalErrorPrediction = ErrorPrediction (labels, HLast) : [2 x *2], [2 x 1 x *2] -> [1]
Validating --> PosteriorProb = Softmax (HLast) : [2 x 1 x *2] -> [2 x 1 x *2]
Validating --> Prior = Mean (labels) : [2 x *2] -> [2]
Validating --> LogOfPrior = Log (Prior) : [2] -> [2]
Validating --> ScaledLogLikelihood = Minus (HLast, LogOfPrior) : [2 x 1 x *2], [2] -> [2 x 1 x *2]

Validating network. 17 nodes to process in pass 2.


Validating network, final pass.



12 out of 25 nodes do not share the minibatch layout with the input data.

Post-processing network complete.



Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 25 matrices, 3 are shared as 1, and 22 are not shared.

	{ CrossEntropyWithSoftmax : [1]
	  EvalErrorPrediction : [1]
	  PosteriorProb : [2 x 1 x *2] }

Minibatch[0]: ActualMBSize = 603
Written to /tmp/cntk-test-20160816095713.701165/Speech_Simple@release_cpu/SimpleOutput*
Total Samples Evaluated = 603

08/16/2016 10:01:47: Action "write" complete.

08/16/2016 10:01:47: __COMPLETED__